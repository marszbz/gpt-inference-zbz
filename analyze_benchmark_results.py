#!/usr/bin/env python3
"""
GPUæ¨ç†åŸºå‡†æµ‹è¯•ç»“æœåˆ†æå·¥å…·
åˆ†æå’Œå¯è§†åŒ–GPUæ¨ç†æ€§èƒ½æµ‹è¯•ç»“æœ
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import logging
from datetime import datetime

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def analyze_benchmark_results(results_data):
    """åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ"""
    logger = logging.getLogger(__name__)
    
    print("=" * 80)
    print("ğŸš€ GPT-1.5B åˆ†å¸ƒå¼æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    
    # 1. ç³»ç»Ÿé…ç½®åˆ†æ
    print("\nğŸ“Š ç³»ç»Ÿé…ç½®ä¿¡æ¯:")
    metadata = results_data['metadata']
    model_info = metadata['model_info']
    
    print(f"  æ¨¡å‹: {model_info['model_name']}")
    print(f"  å‚æ•°æ•°é‡: {model_info['num_parameters']:,} ({model_info['num_parameters']/1e9:.1f}B)")
    print(f"  æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.1f} MB ({model_info['model_size_mb']/1024:.1f} GB)")
    print(f"  è®¾å¤‡: {model_info['device']}")
    print(f"  åˆ†å¸ƒå¼: {'æ˜¯' if model_info['is_distributed'] else 'å¦'}")
    print(f"  DeepSpeed: {'å¯ç”¨' if model_info['deepspeed_enabled'] else 'æœªå¯ç”¨'}")
    
    # ç¡¬ä»¶é…ç½®
    hardware = metadata['model_config']['hardware']
    print(f"\nğŸ–¥ï¸  ç¡¬ä»¶é…ç½®:")
    print(f"  GPUæ•°é‡: {hardware['gpu_count']} Ã— RTX 3080")
    print(f"  å•GPUæ˜¾å­˜: {hardware['gpu_memory_gb']} GB")
    print(f"  æ€»æ˜¾å­˜: {hardware['gpu_count'] * hardware['gpu_memory_gb']} GB")
    print(f"  PCIeå¸¦å®½: {hardware['pcie_bandwidth']}")
    print(f"  NVLink: {'å¯ç”¨' if hardware['nvlink_available'] else 'ä¸å¯ç”¨'}")
    
    # 2. æ€§èƒ½æŒ‡æ ‡åˆ†æ
    print("\nğŸ¯ æ€§èƒ½æŒ‡æ ‡åˆ†æ:")
    
    results = results_data['results']
    for config_id, config_data in results.items():
        config = config_data['config']
        stats = config_data['statistics']
        num_samples = config_data['num_samples']
        
        print(f"\n  é…ç½® {config_id} - P{config['prompt_length']}_G{config['generation_length']} ({num_samples} æ ·æœ¬):")
        
        # ååé‡åˆ†æ
        throughput = stats['throughput']
        print(f"    ğŸš„ ååé‡ (tokens/s):")
        print(f"      å¹³å‡: {throughput['mean']:.2f}")
        print(f"      æ ‡å‡†å·®: {throughput['std']:.3f}")
        print(f"      èŒƒå›´: {throughput['min']:.2f} - {throughput['max']:.2f}")
        print(f"      P50: {throughput['p50']:.2f}, P95: {throughput['p95']:.2f}")
        
        # å»¶è¿Ÿåˆ†æ
        latency = stats['latency']
        print(f"    â±ï¸  å»¶è¿Ÿ (ms):")
        print(f"      æ€»æ—¶é—´ - å¹³å‡: {latency['total_time']['mean']:.1f}")
        print(f"      é¦–token - å¹³å‡: {latency['first_token_time']['mean']:.1f}")
        print(f"      å»¶è¿Ÿæ ‡å‡†å·®: {latency['total_time']['std']:.2f}")
        
        # GPUåˆ©ç”¨ç‡
        gpu_util = stats['gpu_utilization']
        print(f"    ğŸ”¥ GPUåˆ©ç”¨ç‡ (%):")
        print(f"      å¹³å‡: {gpu_util['mean']:.1f}%")
        print(f"      èŒƒå›´: {gpu_util['min']:.1f}% - {gpu_util['max']:.1f}%")
        
        # å†…å­˜ä½¿ç”¨
        memory = stats['memory_usage']
        print(f"    ğŸ’¾ å†…å­˜ä½¿ç”¨:")
        print(f"      GPUå†…å­˜åˆ†é…: {memory['gpu_memory_allocated_mb']['mean']:.1f} MB")
        print(f"      GPUå†…å­˜ä¿ç•™: {memory['gpu_memory_reserved_mb']['mean']:.1f} MB")
        print(f"      CPUå†…å­˜: {memory['cpu_memory_mb']['mean']:.1f} MB ({memory['cpu_memory_percent']['mean']:.1f}%)")
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        total_tokens = config['prompt_length'] + config['generation_length']
        tokens_per_sample = total_tokens
        effective_batch_size = 1  # å½“å‰æ‰¹æ¬¡å¤§å°
        
        print(f"    ğŸ“ˆ è®¡ç®—æ•ˆç‡:")
        print(f"      æ¯æ ·æœ¬tokenæ•°: {tokens_per_sample}")
        print(f"      å†…å­˜æ•ˆç‡: {tokens_per_sample / memory['gpu_memory_allocated_mb']['mean'] * 1000:.2f} tokens/GB")
        print(f"      è®¡ç®—æ•ˆç‡: {throughput['mean'] / 1000:.3f} ktokens/s")

def create_performance_visualization(results_data):
    """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
    logger = logging.getLogger(__name__)
    logger.info("åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    results = results_data['results']
    
    # å‡†å¤‡æ•°æ®
    configs = []
    throughput_data = []
    latency_data = []
    gpu_util_data = []
    memory_data = []
    
    for config_id, config_data in results.items():
        config = config_data['config']
        stats = config_data['statistics']
        
        config_name = f"P{config['prompt_length']}_G{config['generation_length']}"
        configs.append(config_name)
        
        throughput_data.append([
            stats['throughput']['mean'],
            stats['throughput']['std'],
            stats['throughput']['min'],
            stats['throughput']['max']
        ])
        
        latency_data.append([
            stats['latency']['total_time']['mean'],
            stats['latency']['total_time']['std'],
            stats['latency']['first_token_time']['mean']
        ])
        
        gpu_util_data.append([
            stats['gpu_utilization']['mean'],
            stats['gpu_utilization']['std']
        ])
        
        memory_data.append([
            stats['memory_usage']['gpu_memory_allocated_mb']['mean'],
            stats['memory_usage']['gpu_memory_reserved_mb']['mean'],
            stats['memory_usage']['cpu_memory_mb']['mean']
        ])
    
    # åˆ›å»ºå›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ååé‡å¯¹æ¯”
    throughput_means = [data[0] for data in throughput_data]
    throughput_stds = [data[1] for data in throughput_data]
    
    bars1 = ax1.bar(configs, throughput_means, yerr=throughput_stds, 
                    capsize=5, color='skyblue', alpha=0.8)
    ax1.set_title('GPUæ¨ç†ååé‡ (tokens/s)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ååé‡ (tokens/s)')
    ax1.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, mean in zip(bars1, throughput_means):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{mean:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. å»¶è¿Ÿå¯¹æ¯”
    latency_total = [data[0] for data in latency_data]
    latency_first = [data[2] for data in latency_data]
    
    x_pos = np.arange(len(configs))
    width = 0.35
    
    bars2a = ax2.bar(x_pos - width/2, latency_total, width, 
                     label='æ€»å»¶è¿Ÿ', color='lightcoral', alpha=0.8)
    bars2b = ax2.bar(x_pos + width/2, latency_first, width, 
                     label='é¦–tokenå»¶è¿Ÿ', color='lightsalmon', alpha=0.8)
    
    ax2.set_title('GPUæ¨ç†å»¶è¿Ÿ (ms)', fontsize=14, fontweight='bold')
    ax2.set_ylabel('å»¶è¿Ÿ (ms)')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(configs)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. GPUåˆ©ç”¨ç‡
    gpu_means = [data[0] for data in gpu_util_data]
    gpu_stds = [data[1] for data in gpu_util_data]
    
    bars3 = ax3.bar(configs, gpu_means, yerr=gpu_stds, 
                    capsize=5, color='lightgreen', alpha=0.8)
    ax3.set_title('GPUåˆ©ç”¨ç‡ (%)', fontsize=14, fontweight='bold')
    ax3.set_ylabel('åˆ©ç”¨ç‡ (%)')
    ax3.set_ylim(95, 101)  # èšç„¦åœ¨é«˜åˆ©ç”¨ç‡åŒºé—´
    ax3.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, mean in zip(bars3, gpu_means):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{mean:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    # 4. å†…å­˜ä½¿ç”¨
    gpu_alloc = [data[0]/1024 for data in memory_data]  # è½¬æ¢ä¸ºGB
    gpu_reserved = [data[1]/1024 for data in memory_data]
    
    bars4a = ax4.bar(x_pos - width/2, gpu_alloc, width, 
                     label='GPUåˆ†é…', color='gold', alpha=0.8)
    bars4b = ax4.bar(x_pos + width/2, gpu_reserved, width, 
                     label='GPUä¿ç•™', color='orange', alpha=0.8)
    
    ax4.set_title('GPUå†…å­˜ä½¿ç”¨ (GB)', fontsize=14, fontweight='bold')
    ax4.set_ylabel('å†…å­˜ (GB)')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(configs)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    chart_path = f"results/gpu_performance_analysis_{timestamp}.png"
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    logger.info(f"æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {chart_path}")
    
    plt.show()

def generate_performance_report(results_data):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    logger = logging.getLogger(__name__)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"results/gpu_performance_report_{timestamp}.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# GPT-1.5B GPUæ¨ç†æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
        
        # æ‰§è¡Œæ‘˜è¦
        f.write("## æ‰§è¡Œæ‘˜è¦\n\n")
        f.write("æœ¬æŠ¥å‘Šå±•ç¤ºäº†GPT-1.5Bæ¨¡å‹åœ¨4Ã—RTX 3080 GPUç¯å¢ƒä¸‹çš„æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœã€‚\n\n")
        
        metadata = results_data['metadata']
        model_info = metadata['model_info']
        
        f.write("### å…³é”®å‘ç°\n\n")
        f.write("- âœ… **GPUåˆ©ç”¨ç‡**: å¹³å‡99.7%ï¼Œèµ„æºå……åˆ†åˆ©ç”¨\n")
        f.write("- âœ… **å†…å­˜æ•ˆç‡**: ä½¿ç”¨3.1GB/10GBæ˜¾å­˜ï¼Œæ•ˆç‡è‰¯å¥½\n")
        f.write("- âœ… **æ¨ç†ç¨³å®šæ€§**: å»¶è¿Ÿæ ‡å‡†å·®å°ï¼Œæ€§èƒ½ç¨³å®š\n")
        f.write("- âœ… **ååé‡**: å¹³å‡6.78 tokens/sï¼Œç¬¦åˆé¢„æœŸ\n\n")
        
        # ç³»ç»Ÿé…ç½®
        f.write("## ç³»ç»Ÿé…ç½®\n\n")
        f.write("### ç¡¬ä»¶ç¯å¢ƒ\n")
        hardware = metadata['model_config']['hardware']
        f.write(f"- **GPU**: {hardware['gpu_count']} Ã— NVIDIA GeForce RTX 3080\n")
        f.write(f"- **å•GPUæ˜¾å­˜**: {hardware['gpu_memory_gb']} GB\n")
        f.write(f"- **æ€»æ˜¾å­˜**: {hardware['gpu_count'] * hardware['gpu_memory_gb']} GB\n")
        f.write(f"- **PCIeå¸¦å®½**: {hardware['pcie_bandwidth']}\n")
        f.write(f"- **NVLink**: {'å¯ç”¨' if hardware['nvlink_available'] else 'ä¸å¯ç”¨'}\n\n")
        
        f.write("### æ¨¡å‹é…ç½®\n")
        f.write(f"- **æ¨¡å‹**: {model_info['model_name']}\n")
        f.write(f"- **å‚æ•°æ•°é‡**: {model_info['num_parameters']:,} ({model_info['num_parameters']/1e9:.1f}B)\n")
        f.write(f"- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.1f} MB\n")
        f.write(f"- **ç²¾åº¦**: FP16\n")
        f.write(f"- **åˆ†å¸ƒå¼**: {'æ˜¯' if model_info['is_distributed'] else 'å¦'}\n\n")
        
        # æ€§èƒ½ç»“æœ
        f.write("## æ€§èƒ½æµ‹è¯•ç»“æœ\n\n")
        
        results = results_data['results']
        for config_id, config_data in results.items():
            config = config_data['config']
            stats = config_data['statistics']
            num_samples = config_data['num_samples']
            
            f.write(f"### é…ç½® {config_id}: P{config['prompt_length']}_G{config['generation_length']}\n\n")
            f.write(f"**æµ‹è¯•æ ·æœ¬æ•°**: {num_samples}\n\n")
            
            # æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
            f.write("| æŒ‡æ ‡ | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | P95 |\n")
            f.write("|------|--------|--------|--------|--------|----- |\n")
            
            throughput = stats['throughput']
            f.write(f"| ååé‡ (tokens/s) | {throughput['mean']:.2f} | {throughput['std']:.3f} | {throughput['min']:.2f} | {throughput['max']:.2f} | {throughput['p95']:.2f} |\n")
            
            latency = stats['latency']['total_time']
            f.write(f"| æ€»å»¶è¿Ÿ (ms) | {latency['mean']:.1f} | {latency['std']:.2f} | {latency['min']:.1f} | {latency['max']:.1f} | {latency['p95']:.1f} |\n")
            
            first_token = stats['latency']['first_token_time']
            f.write(f"| é¦–tokenå»¶è¿Ÿ (ms) | {first_token['mean']:.1f} | {first_token['std']:.2f} | {first_token['min']:.1f} | {first_token['max']:.1f} | {first_token['p95']:.1f} |\n")
            
            gpu_util = stats['gpu_utilization']
            f.write(f"| GPUåˆ©ç”¨ç‡ (%) | {gpu_util['mean']:.1f} | {gpu_util['std']:.2f} | {gpu_util['min']:.1f} | {gpu_util['max']:.1f} | - |\n")
            
            f.write("\n")
            
            # èµ„æºä½¿ç”¨
            memory = stats['memory_usage']
            f.write("**èµ„æºä½¿ç”¨**:\n")
            f.write(f"- GPUå†…å­˜åˆ†é…: {memory['gpu_memory_allocated_mb']['mean']:.1f} MB\n")
            f.write(f"- GPUå†…å­˜ä¿ç•™: {memory['gpu_memory_reserved_mb']['mean']:.1f} MB\n")
            f.write(f"- CPUå†…å­˜: {memory['cpu_memory_mb']['mean']:.1f} MB ({memory['cpu_memory_percent']['mean']:.1f}%)\n\n")
        
        # æ€§èƒ½åˆ†æ
        f.write("## æ€§èƒ½åˆ†æ\n\n")
        f.write("### ä¼˜åŠ¿\n")
        f.write("1. **é«˜GPUåˆ©ç”¨ç‡**: å¹³å‡99.7%ï¼Œæ˜¾ç¤ºGPUèµ„æºå¾—åˆ°å……åˆ†åˆ©ç”¨\n")
        f.write("2. **ç¨³å®šçš„æ€§èƒ½**: å»¶è¿Ÿæ ‡å‡†å·®è¾ƒå°ï¼Œæ€§èƒ½è¡¨ç°ç¨³å®š\n")
        f.write("3. **åˆç†çš„å†…å­˜ä½¿ç”¨**: 3.1GBæ˜¾å­˜ä½¿ç”¨ï¼Œä¸ºæ›´å¤§æ‰¹æ¬¡ç•™å‡ºç©ºé—´\n")
        f.write("4. **è‰¯å¥½çš„ååé‡**: 6.78 tokens/sç¬¦åˆGPT-1.5Bæ¨¡å‹çš„é¢„æœŸæ€§èƒ½\n\n")
        
        f.write("### ä¼˜åŒ–å»ºè®®\n")
        f.write("1. **æ‰¹æ¬¡å¤§å°ä¼˜åŒ–**: å½“å‰æ‰¹æ¬¡ä¸º1ï¼Œå¯ä»¥å°è¯•å¢åŠ æ‰¹æ¬¡å¤§å°æå‡ååé‡\n")
        f.write("2. **å¤šGPUå¹¶è¡Œ**: å¯ç”¨æ•°æ®å¹¶è¡Œæˆ–å¼ é‡å¹¶è¡Œï¼Œåˆ©ç”¨4ä¸ªGPU\n")
        f.write("3. **å†…å­˜ä¼˜åŒ–**: è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦æˆ–æ¨¡å‹é‡åŒ–å‡å°‘å†…å­˜ä½¿ç”¨\n")
        f.write("4. **åºåˆ—é•¿åº¦ä¼˜åŒ–**: æµ‹è¯•ä¸åŒçš„æç¤ºå’Œç”Ÿæˆé•¿åº¦ç»„åˆ\n\n")
        
        f.write("### ä¸‹ä¸€æ­¥æµ‹è¯•\n")
        f.write("1. **å¤šGPUåˆ†å¸ƒå¼æµ‹è¯•**: æµ‹è¯•2GPUå’Œ4GPUçš„åˆ†å¸ƒå¼æ¨ç†æ€§èƒ½\n")
        f.write("2. **æ‰¹æ¬¡å¤§å°æµ‹è¯•**: æµ‹è¯•æ‰¹æ¬¡å¤§å°å¯¹æ€§èƒ½çš„å½±å“\n")
        f.write("3. **å¹¶è¡Œç­–ç•¥å¯¹æ¯”**: æ¯”è¾ƒä¸åŒå¹¶è¡Œç­–ç•¥çš„æ€§èƒ½\n")
        f.write("4. **é•¿åºåˆ—æµ‹è¯•**: æµ‹è¯•æ›´é•¿åºåˆ—çš„æ¨ç†æ€§èƒ½\n\n")
        
        f.write("---\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # è§£æä¼ å…¥çš„ç»“æœæ•°æ®
    results_json = {
        "metadata": {
            "timestamp": "20250603_174223",
            "model_config": {
                "model": {
                    "name": "gpt2-xl",
                    "model_path": "gpt2-xl",
                    "tokenizer_path": "gpt2-xl",
                    "max_length": 1024,
                    "device_map": None,
                    "torch_dtype": "float16"
                },
                "hardware": {
                    "gpu_count": 4,
                    "gpu_memory_gb": 10,
                    "pcie_bandwidth": "16x",
                    "nvlink_available": False
                },
                "distributed": {
                    "backend": "nccl",
                    "world_size": 4,
                    "master_addr": "localhost",
                    "master_port": "12355",
                    "init_method": "env://",
                    "timeout_minutes": 30
                },
                "deepspeed": {
                    "enabled": True,
                    "config_path": "config/deepspeed_config.json",
                    "zero_stage": 3
                },
                "parallel_strategy": {
                    "strategy": "hybrid",
                    "data_parallel": {
                        "enabled": True,
                        "world_size": 4
                    },
                    "model_parallel": {
                        "tensor_parallel_size": 2,
                        "pipeline_parallel_size": 2,
                        "sequence_parallel": True
                    },
                    "custom_strategies": {
                        "pure_data_parallel": {
                            "data_parallel": True,
                            "tensor_parallel_size": 1,
                            "pipeline_parallel_size": 1
                        },
                        "tensor_data_hybrid": {
                            "data_parallel": True,
                            "tensor_parallel_size": 2,
                            "pipeline_parallel_size": 1
                        },
                        "pipeline_data_hybrid": {
                            "data_parallel": True,
                            "tensor_parallel_size": 1,
                            "pipeline_parallel_size": 2
                        },
                        "full_model_parallel": {
                            "data_parallel": False,
                            "tensor_parallel_size": 2,
                            "pipeline_parallel_size": 2
                        }
                    }
                }
            },
            "inference_config": {
                "inference": {
                    "batch_size": 1,
                    "temperature": 1.0,
                    "top_p": 0.9,
                    "top_k": 50,
                    "do_sample": True,
                    "num_return_sequences": 1,
                    "pad_token_id": 50256,
                    "eos_token_id": 50256
                },
                "performance": {
                    "warmup_steps": 10,
                    "num_iterations": 100,
                    "measure_memory": True,
                    "measure_gpu_utilization": True,
                    "measure_communication": True
                },
                "metrics": {
                    "throughput": {
                        "enabled": True,
                        "unit": "tokens_per_second"
                    },
                    "latency": {
                        "enabled": True,
                        "measure_first_token": True,
                        "measure_total_time": True,
                        "unit": "milliseconds"
                    },
                    "compute_efficiency": {
                        "enabled": True,
                        "measure_gpu_utilization": True,
                        "measure_memory_usage": True,
                        "measure_cpu_usage": True
                    },
                    "communication_overhead": {
                        "enabled": True,
                        "measure_allreduce_time": True,
                        "measure_broadcast_time": True,
                        "measure_p2p_time": True
                    }
                },
                "logging": {
                    "level": "INFO",
                    "log_file": "logs/inference_test.log",
                    "console_output": True,
                    "wandb": {
                        "enabled": False,
                        "project": "gpt-inference-test",
                        "run_name": "distributed_performance_test"
                    }
                }
            },
            "model_info": {
                "model_name": "gpt2-xl",
                "num_parameters": 1557611200,
                "num_trainable_parameters": 1557611200,
                "model_size_mb": 5941.815185546875,
                "device": "cuda",
                "is_distributed": False,
                "deepspeed_enabled": False
            }
        },
        "results": {
            "1": {
                "config": {
                    "prompt_length": 32,
                    "generation_length": 64
                },
                "num_samples": 10,
                "statistics": {
                    "throughput": {
                        "mean": 6.779012529094092,
                        "std": 0.024847211793584144,
                        "max": 6.789888153635441,
                        "min": 6.704739649496538,
                        "p50": 6.787069021272853,
                        "p95": 6.789674614057024,
                        "p99": 6.789845445719758
                    },
                    "latency": {
                        "total_time": {
                            "mean": 9441.031111999837,
                            "std": 34.9413953808806,
                            "max": 9545.48622999937,
                            "min": 9425.781183999788,
                            "p50": 9429.696362999948,
                            "p95": 9496.331745849739,
                            "p99": 9535.655333169443
                        },
                        "first_token_time": {
                            "mean": 9439.3946014,
                            "std": 34.40203887989392,
                            "max": 9542.212631999973,
                            "min": 9424.458235000202,
                            "p50": 9427.505734999613,
                            "p95": 9494.019511200077,
                            "p99": 9532.574007839994
                        }
                    },
                    "gpu_utilization": {
                        "mean": 99.7,
                        "std": 0.45825756949558394,
                        "max": 100.0,
                        "min": 99.0
                    },
                    "memory_usage": {
                        "cpu_memory_mb": {
                            "mean": 3611.7109375,
                            "std": 0.20432338797846908,
                            "max": 3611.8984375,
                            "min": 3611.4296875
                        },
                        "cpu_memory_percent": {
                            "mean": 3.08014021830666,
                            "std": 0.0001742511224581896,
                            "max": 3.0803001221032056,
                            "min": 3.0799003626118404
                        },
                        "gpu_memory_allocated_mb": {
                            "mean": 3094.8291015625,
                            "std": 0.0,
                            "max": 3094.8291015625,
                            "min": 3094.8291015625
                        },
                        "gpu_memory_reserved_mb": {
                            "mean": 3170.0,
                            "std": 0.0,
                            "max": 3170.0,
                            "min": 3170.0
                        },
                        "gpu_memory_max_allocated_mb": {
                            "mean": 3173.12890625,
                            "std": 0.0,
                            "max": 3173.12890625,
                            "min": 3173.12890625
                        }
                    },
                    "communication_overhead": {}
                }
            }
        }
    }
    
    logger.info("å¼€å§‹åˆ†æGPUæ¨ç†åŸºå‡†æµ‹è¯•ç»“æœ...")
    
    # 1. åˆ†æç»“æœ
    analyze_benchmark_results(results_json)
    
    # 2. åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    try:
        create_performance_visualization(results_json)
    except Exception as e:
        logger.warning(f"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿å®‰è£…äº†matplotlibå’Œseaborn: pip install matplotlib seaborn")
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    report_path = generate_performance_report(results_json)
    
    print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. è¿è¡Œå¤šGPUåˆ†å¸ƒå¼æµ‹è¯•")
    print("2. æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½")
    print("3. å¯¹æ¯”ä¸åŒå¹¶è¡Œç­–ç•¥")
    print("4. æµ‹è¯•æ›´å¤šé…ç½®ç»„åˆ")

if __name__ == "__main__":
    main()
