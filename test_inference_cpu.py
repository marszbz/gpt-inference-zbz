#!/usr/bin/env python3
"""
CPUå…¼å®¹æ€§æ¨ç†æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ç³»ç»Ÿåœ¨CPUç¯å¢ƒä¸‹çš„åŸºæœ¬åŠŸèƒ½
"""

import sys
import os
import logging
import torch
from pathlib import Path
import json
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def test_cpu_compatibility():
    """æµ‹è¯•CPUå…¼å®¹æ€§"""
    logger = logging.getLogger(__name__)
    
    logger.info("=== CPUå…¼å®¹æ€§æµ‹è¯•å¼€å§‹ ===")
    
    # 1. æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    logger.info("1. æ£€æŸ¥è®¡ç®—è®¾å¤‡...")
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        logger.info(f"âœ“ CUDAå¯ç”¨ï¼Œæ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        device = torch.device("cuda:0")
    else:
        logger.info("âœ“ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        device = torch.device("cpu")
    
    # 2. æµ‹è¯•åŸºæœ¬æ¨¡å—å¯¼å…¥
    logger.info("2. æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    try:
        from src.models import ModelManager
        from src.inference import DistributedInferenceEngine
        from src.utils import DataLoader, PerformanceMonitor
        logger.info("âœ“ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        logger.error(f"âœ— æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•æ•°æ®åŠ è½½
    logger.info("3. æµ‹è¯•æ•°æ®åŠ è½½...")
    try:
        data_loader = DataLoader("data/datasets")
        samples = data_loader.load_samples(config_ids=[0])  # åªåŠ è½½é…ç½®0
        if samples:
            logger.info(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬ä¿¡æ¯
            sample = samples[0]
            logger.info(f"  æ ·æœ¬ç¤ºä¾‹: prompt_length={sample.prompt_length}, "
                       f"generation_length={sample.generation_length}, source={sample.source_type}")
        else:
            logger.warning("! æ²¡æœ‰åŠ è½½åˆ°æ•°æ®æ ·æœ¬")
    except Exception as e:
        logger.error(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 4. æµ‹è¯•ç®€å•çš„Transformeræ¨¡å‹åŠ è½½ï¼ˆä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼‰
    logger.info("4. æµ‹è¯•è½»é‡çº§æ¨¡å‹åŠ è½½...")
    try:
        from transformers import GPT2LMHeadModel, GPT2Tokenizer
        
        # ä½¿ç”¨GPT2-smallè¿›è¡Œæµ‹è¯•ï¼ˆæ›´é€‚åˆCPUï¼‰
        model_name = "gpt2"  # 124Må‚æ•°ï¼Œé€‚åˆCPUæµ‹è¯•
        
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        model = GPT2LMHeadModel.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = model.to(device)
        model.eval()
        
        logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 5. æµ‹è¯•ç®€å•æ¨ç†
        logger.info("5. æµ‹è¯•ç®€å•æ¨ç†...")
        test_prompt = "The future of artificial intelligence is"
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer.encode(test_prompt, return_tensors="pt").to(device)
        
        # ç”Ÿæˆæ–‡æœ¬
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=10,  # å°‘é‡tokenä»¥åŠ å¿«é€Ÿåº¦
                do_sample=True,
                temperature=0.8,
                pad_token_id=tokenizer.pad_token_id
            )
        
        inference_time = time.time() - start_time
        
        # è§£ç è¾“å‡º
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        logger.info(f"âœ“ æ¨ç†æˆåŠŸ")
        logger.info(f"  è¾“å…¥: {test_prompt}")
        logger.info(f"  è¾“å‡º: {generated_text}")
        logger.info(f"  æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        
    except Exception as e:
        logger.error(f"âœ— æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        logger.warning("è¿™å¯èƒ½æ˜¯å› ä¸ºç½‘ç»œé—®é¢˜æˆ–æ¨¡å‹ä¸‹è½½å¤±è´¥")
        logger.info("æç¤º: å¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•")
        return False
    
    # 6. æµ‹è¯•æ€§èƒ½ç›‘æ§
    logger.info("6. æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    try:
        monitor = PerformanceMonitor()
        system_info = monitor.get_system_info()
        logger.info("âœ“ æ€§èƒ½ç›‘æ§æ­£å¸¸")
        logger.info(f"  CPUæ ¸å¿ƒæ•°: {system_info.get('cpu_count', 'N/A')}")
        logger.info(f"  å†…å­˜: {system_info.get('memory_total_gb', 'N/A'):.1f}GB")
        if cuda_available:
            logger.info(f"  GPU: {system_info.get('gpu_info', {}).get('name', 'N/A')}")
    except Exception as e:
        logger.error(f"âœ— æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    logger.info("=== CPUå…¼å®¹æ€§æµ‹è¯•å®Œæˆ ===")
    logger.info("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿå¯ä»¥åœ¨CPUç¯å¢ƒä¸‹è¿è¡Œ")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    success = test_cpu_compatibility()
    
    if success:
        print("\nğŸ‰ CPUå…¼å®¹æ€§æµ‹è¯•æˆåŠŸï¼")
        print("ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„æ¨ç†æµ‹è¯•è„šæœ¬äº†")
        return 0
    else:
        print("\nâŒ CPUå…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜")
        return 1

if __name__ == "__main__":
    sys.exit(main())
