#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤šGPUæ¨ç†æµ‹è¯•
æµ‹è¯•æ•°æ®å¹¶è¡Œç­–ç•¥çš„æ€§èƒ½
"""

import os
import sys
import json
import torch
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_single_vs_multi_gpu():
    """å¯¹æ¯”å•GPUå’Œå¤šGPUæ€§èƒ½"""
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    if not torch.cuda.is_available():
        print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°CUDA GPU")
        return
    
    gpu_count = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "å•GPU", "use_multi_gpu": False, "batch_size": 4},
    ]
    
    if gpu_count >= 2:
        test_configs.append({"name": "å¤šGPUæ•°æ®å¹¶è¡Œ", "use_multi_gpu": True, "batch_size": 8})
    
    # åŠ è½½æµ‹è¯•æ ·æœ¬
    from src.utils.data_loader import DataLoader
    
    dataset_files = list(Path("data/datasets").glob("benchmark_dataset_config_*.jsonl"))
    if not dataset_files:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶")
        return
    
    # åŠ è½½ç¬¬ä¸€ä¸ªé…ç½®çš„æ•°æ®
    data_loader = DataLoader(str(dataset_files[0]))
    samples = data_loader.load_samples()[:20]  # ä½¿ç”¨20ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    
    print(f"åŠ è½½äº† {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    results = []
    
    for config in test_configs:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•é…ç½®: {config['name']}")
        print(f"æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"{'='*50}")
        
        try:
            # å¯¼å…¥æ¨¡å‹ç®¡ç†å™¨
            from src.models.model_manager import ModelManager
            
            # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
            model_manager = ModelManager()
            
            # åŠ è½½æ¨¡å‹
            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            model_manager.load_model()
            
            # å¦‚æœä½¿ç”¨å¤šGPUï¼ŒåŒ…è£…æ¨¡å‹
            if config["use_multi_gpu"] and gpu_count > 1:
                model_manager.model = torch.nn.DataParallel(
                    model_manager.model, 
                    device_ids=list(range(min(gpu_count, 4)))  # æœ€å¤šä½¿ç”¨4ä¸ªGPU
                )
                print(f"å¯ç”¨DataParallelï¼Œä½¿ç”¨ {min(gpu_count, 4)} ä¸ªGPU")
            
            # é¢„çƒ­
            print("æ¨¡å‹é¢„çƒ­ä¸­...")
            with torch.no_grad():
                for i in range(3):
                    test_prompt = "This is a test prompt for warmup"
                    inputs = model_manager.prepare_inputs([test_prompt])
                    _ = model_manager.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_new_tokens=10
                    )
            
            # å¼€å§‹æ€§èƒ½æµ‹è¯•
            batch_size = config["batch_size"]
            total_tokens = 0
            total_samples = 0
            
            start_time = time.time()
            
            # æŒ‰æ‰¹æ¬¡å¤„ç†æ ·æœ¬
            for i in range(0, len(samples), batch_size):
                batch_samples = samples[i:i+batch_size]
                batch_prompts = []
                
                for sample in batch_samples:
                    prompt = sample.prompt if hasattr(sample, 'prompt') else sample['prompt']
                    batch_prompts.append(prompt)
                
                batch_start = time.time()
                
                # æ‰¹é‡æ¨ç†
                with torch.no_grad():
                    for prompt in batch_prompts:
                        inputs = model_manager.prepare_inputs([prompt])
                        outputs = model_manager.generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_new_tokens=32,
                            do_sample=True,
                            temperature=0.8,
                            top_p=0.9
                        )
                        
                        # è®¡ç®—ç”Ÿæˆçš„tokenæ•°é‡
                        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                        total_tokens += len(generated_tokens)
                        total_samples += 1
                
                batch_time = time.time() - batch_start
                print(f"æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_prompts)} æ ·æœ¬, ç”¨æ—¶ {batch_time:.2f}s")
            
            total_time = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            throughput = total_tokens / total_time
            avg_latency = total_time / total_samples
            samples_per_sec = total_samples / total_time
            
            # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            gpu_memory_mb = torch.cuda.memory_allocated() / (1024 * 1024)
            
            result = {
                "config": config,
                "performance": {
                    "total_time": total_time,
                    "total_tokens": total_tokens,
                    "total_samples": total_samples,
                    "throughput_tokens_per_sec": throughput,
                    "avg_latency_sec": avg_latency,
                    "samples_per_sec": samples_per_sec,
                    "gpu_memory_mb": gpu_memory_mb
                },
                "timestamp": datetime.now().isoformat()
            }
            
            results.append(result)
            
            print(f"\nâœ… {config['name']} æµ‹è¯•å®Œæˆ:")
            print(f"   æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"   ååé‡: {throughput:.2f} tokens/s")
            print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.3f}s")
            print(f"   æ ·æœ¬å¤„ç†é€Ÿåº¦: {samples_per_sec:.2f} samples/s")
            print(f"   GPUå†…å­˜ä½¿ç”¨: {gpu_memory_mb:.1f} MB")
            
            # æ¸…ç†å†…å­˜
            del model_manager
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ {config['name']} æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if len(results) >= 2:
        print(f"\n{'='*70}")
        print("ğŸš€ å¤šGPUæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print(f"{'='*70}")
        
        single_gpu_result = results[0]
        multi_gpu_result = results[1]
        
        single_throughput = single_gpu_result["performance"]["throughput_tokens_per_sec"]
        multi_throughput = multi_gpu_result["performance"]["throughput_tokens_per_sec"]
        
        speedup = multi_throughput / single_throughput
        efficiency = speedup / gpu_count * 100
        
        print(f"å•GPUååé‡:     {single_throughput:.2f} tokens/s")
        print(f"å¤šGPUååé‡:     {multi_throughput:.2f} tokens/s")
        print(f"åŠ é€Ÿæ¯”:         {speedup:.2f}x")
        print(f"å¹¶è¡Œæ•ˆç‡:       {efficiency:.1f}%")
        
        single_latency = single_gpu_result["performance"]["avg_latency_sec"]
        multi_latency = multi_gpu_result["performance"]["avg_latency_sec"]
        latency_improvement = (single_latency - multi_latency) / single_latency * 100
        
        print(f"å•GPUå¹³å‡å»¶è¿Ÿ:   {single_latency:.3f}s")
        print(f"å¤šGPUå¹³å‡å»¶è¿Ÿ:   {multi_latency:.3f}s")
        print(f"å»¶è¿Ÿæ”¹å–„:       {latency_improvement:.1f}%")
    
    # ä¿å­˜ç»“æœ
    results_file = f"results/multi_gpu_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("results", exist_ok=True)
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == "__main__":
    print("ğŸ”¥ å¼€å§‹å¤šGPUæ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    test_single_vs_multi_gpu()
    print("âœ… æµ‹è¯•å®Œæˆ")
