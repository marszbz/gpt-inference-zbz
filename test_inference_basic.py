#!/usr/bin/env python3
"""
åŸºç¡€æ¨ç†åŠŸèƒ½æµ‹è¯•
éªŒè¯æ¨ç†å¼•æ“çš„åŸºæœ¬åŠŸèƒ½ï¼Œæ— éœ€GPUç¯å¢ƒ
"""

import sys
import os
import logging
from pathlib import Path
import json
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½ ===")
    
    try:
        from src.utils import DataLoader
        
        # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
        dataset_files = list(Path("data/datasets").glob("benchmark_dataset_config_*.jsonl"))
        if not dataset_files:
            logger.error("æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶")
            return False
        
        # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªé…ç½®æ–‡ä»¶
        test_file = dataset_files[0]
        logger.info(f"æµ‹è¯•åŠ è½½æ•°æ®é›†: {test_file}")
        
        data_loader = DataLoader(str(test_file))
        samples = data_loader.load_samples()
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
        
        # éªŒè¯æ ·æœ¬æ ¼å¼
        if samples:
            sample = samples[0]
            logger.info(f"æ ·æœ¬ç¤ºä¾‹ - ID: {sample.id}, é…ç½®: {sample.config_id}, "
                       f"æç¤ºé•¿åº¦: {sample.prompt_length}, ç”Ÿæˆé•¿åº¦: {sample.generation_length}")
        
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_manager_loading():
    """æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨åŠ è½½"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨åŠ è½½ ===")
    
    try:
        from src.models import ModelManager
        
        # æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½
        model_manager = ModelManager("config/model_config.yaml")
        logger.info("æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ¨¡å‹é…ç½®
        config = model_manager.config
        logger.info(f"æ¨¡å‹åç§°: {config['model']['name']}")
        logger.info(f"æ¨¡å‹è·¯å¾„: {config['model']['path']}")
        logger.info(f"ç¼“å­˜ç›®å½•: {config['model']['cache_dir']}")
        
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_inference_engine_init():
    """æµ‹è¯•æ¨ç†å¼•æ“åˆå§‹åŒ–"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•æ¨ç†å¼•æ“åˆå§‹åŒ– ===")
    
    try:
        from src.models import ModelManager
        from src.inference import DistributedInferenceEngine
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager("config/model_config.yaml")
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        inference_engine = DistributedInferenceEngine(
            model_manager, 
            "config/inference_config.yaml"
        )
        
        logger.info("æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨
        performance_monitor = inference_engine.performance_monitor
        system_info = performance_monitor.get_system_info()
        
        logger.info(f"ç³»ç»Ÿä¿¡æ¯è·å–æˆåŠŸ: CPUæ ¸å¿ƒæ•° = {system_info['cpu']['cores']}")
        
        return True
        
    except Exception as e:
        logger.error(f"æ¨ç†å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance_evaluator():
    """æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•æ€§èƒ½è¯„ä¼°å™¨ ===")
    
    try:
        from src.evaluation import PerformanceEvaluator
        
        # åˆå§‹åŒ–æ€§èƒ½è¯„ä¼°å™¨
        evaluator = PerformanceEvaluator("config/inference_config.yaml")
        logger.info("æ€§èƒ½è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæ¨¡æ‹Ÿç»“æœæ•°æ®
        mock_results = [
            {
                'sample_id': 'test_1',
                'total_time': 100.0,
                'first_token_time': 50.0,
                'throughput': 25.0,
                'prompt_tokens': 32,
                'generated_tokens': 32,
                'memory_usage': {'gpu_memory_allocated_mb': 1024.0},
                'gpu_utilization': 85.0
            },
            {
                'sample_id': 'test_2',
                'total_time': 120.0,
                'first_token_time': 60.0,
                'throughput': 22.0,
                'prompt_tokens': 128,
                'generated_tokens': 64,
                'memory_usage': {'gpu_memory_allocated_mb': 1200.0},
                'gpu_utilization': 90.0
            }
        ]
        
        # æµ‹è¯•ç»Ÿè®¡è®¡ç®—
        stats = evaluator.calculate_statistics(mock_results)
        logger.info(f"ç»Ÿè®¡è®¡ç®—æˆåŠŸ: å¹³å‡å»¶è¿Ÿ = {stats['latency']['total_time']['mean']:.2f} ms")
        
        return True
        
    except Exception as e:
        logger.error(f"æ€§èƒ½è¯„ä¼°å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_parallel_strategy_manager():
    """æµ‹è¯•å¹¶è¡Œç­–ç•¥ç®¡ç†å™¨"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•å¹¶è¡Œç­–ç•¥ç®¡ç†å™¨ ===")
    
    try:
        from src.models import ParallelStrategyManager
        
        # åˆå§‹åŒ–ç­–ç•¥ç®¡ç†å™¨
        strategy_manager = ParallelStrategyManager()
        logger.info("å¹¶è¡Œç­–ç•¥ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç­–ç•¥é€‰æ‹©
        strategy = strategy_manager.select_strategy(
            model_size=1.5,
            num_gpus=1,
            memory_per_gpu=8
        )
        
        logger.info(f"ç­–ç•¥é€‰æ‹©æˆåŠŸ: {strategy['name']}")
        logger.info(f"ç­–ç•¥æè¿°: {strategy['description']}")
        
        return True
        
    except Exception as e:
        logger.error(f"å¹¶è¡Œç­–ç•¥ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config_files():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•é…ç½®æ–‡ä»¶ ===")
    
    config_files = [
        "config/model_config.yaml",
        "config/data_config.yaml", 
        "config/inference_config.yaml",
        "config/deepspeed_config.json"
    ]
    
    all_valid = True
    
    for config_file in config_files:
        try:
            if config_file.endswith('.yaml'):
                import yaml
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            else:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            
            logger.info(f"âœ“ {config_file} åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âœ— {config_file} åŠ è½½å¤±è´¥: {e}")
            all_valid = False
    
    return all_valid

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹åŸºç¡€æ¨ç†åŠŸèƒ½æµ‹è¯•")
    
    tests = [
        ("é…ç½®æ–‡ä»¶", test_config_files),
        ("æ•°æ®åŠ è½½", test_data_loading),
        ("æ¨¡å‹ç®¡ç†å™¨", test_model_manager_loading),
        ("å¹¶è¡Œç­–ç•¥ç®¡ç†å™¨", test_parallel_strategy_manager),
        ("æ¨ç†å¼•æ“åˆå§‹åŒ–", test_inference_engine_init),
        ("æ€§èƒ½è¯„ä¼°å™¨", test_performance_evaluator),
    ]
    
    results = {}
    start_time = time.time()
    
    for test_name, test_func in tests:
        logger.info(f"\n--- å¼€å§‹æµ‹è¯•: {test_name} ---")
        try:
            result = test_func()
            results[test_name] = result
            status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
            logger.info(f"æµ‹è¯•ç»“æœ: {status}")
        except Exception as e:
            results[test_name] = False
            logger.error(f"æµ‹è¯•å¼‚å¸¸: {e}")
    
    end_time = time.time()
    
    # è¾“å‡ºæ€»ç»“
    logger.info("\n" + "="*50)
    logger.info("åŸºç¡€æ¨ç†åŠŸèƒ½æµ‹è¯•æ€»ç»“")
    logger.info("="*50)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        logger.info(f"{test_name:20s}: {status}")
    
    logger.info(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    logger.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        logger.warning(f"âš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    setup_logging()
    
    try:
        success = run_comprehensive_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"æµ‹è¯•å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        sys.exit(1)
