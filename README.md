# GPT-1.5B Distributed Inference Performance Testing System

ä¸€ä¸ªå…¨é¢çš„ GPT-1.5B åˆ†å¸ƒå¼æ¨ç†æ€§èƒ½æµ‹è¯•ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§å¹¶è¡Œç­–ç•¥å’Œæ·±åº¦æ€§èƒ½åˆ†æã€‚å·²å®Œæˆåœ¨ RTX 3080 GPU ä¸Šçš„å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå®ç°æœ€é«˜ 2.57 å€åŠ é€Ÿæ¯”å’Œ 94.7%GPU åˆ©ç”¨ç‡ã€‚

## ğŸŒŸ ä¸»è¦ç‰¹æ€§

- ğŸš€ **åˆ†å¸ƒå¼æ¨ç†å¼•æ“**: åŸºäº PyTorch DistributedDataParallel çš„é«˜æ•ˆåˆ†å¸ƒå¼æ¨ç†
- âš¡ **å››ç§å¹¶è¡Œç­–ç•¥**: Pure Data Parallelã€Tensor Data Hybridã€Pipeline Data Hybridã€Full Model Parallel
- ğŸ“ˆ **å…¨é¢æ€§èƒ½ç›‘æ§**: ååé‡ã€å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨ã€GPU åˆ©ç”¨ç‡ã€å¹¶è¡Œæ•ˆç‡ç­‰
- ğŸ¯ **RTX 3080 ä¼˜åŒ–**: é’ˆå¯¹ 10GB æ˜¾å­˜çš„ä¸“é¡¹ä¼˜åŒ–ï¼Œæ”¯æŒ 1-4 å¡é…ç½®
- ğŸ“Š **è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•**: ä¸€é”®è¿è¡Œ 9 ç§é…ç½®ç»„åˆçš„å®Œæ•´æ€§èƒ½æµ‹è¯•
- ğŸ“‹ **è¯¦ç»†æ€§èƒ½æŠ¥å‘Š**: è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨ã€è¡¨æ ¼å’Œç»¼åˆåˆ†ææŠ¥å‘Š
- ğŸ”§ **æ•…éšœæ’é™¤**: å†…ç½® GPU è¯Šæ–­å’Œ CUDA è®¾å¤‡æ˜ å°„ä¿®å¤

## ğŸ“ é¡¹ç›®ç»“æ„

```text
gpt_inference_zbz/
â”œâ”€â”€ requirements.txt          # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ config/                   # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ model_config.yaml    # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ data_config.yaml     # æ•°æ®é…ç½®
â”‚   â”œâ”€â”€ inference_config.yaml # æ¨ç†é…ç½®
â”‚   â””â”€â”€ deepspeed_config.json # DeepSpeedé…ç½®
â”œâ”€â”€ data/                     # æ•°æ®ç›¸å…³
â”‚   â”œâ”€â”€ test_prompts.jsonl   # æ ‡å‡†æµ‹è¯•æ•°æ®é›†(20æ¡æ ·æœ¬)
â”‚   â”œâ”€â”€ datasets/            # ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•æ•°æ®é›†
â”‚   â”œâ”€â”€ processed/           # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ raw/                 # åŸå§‹æ•°æ®
â”œâ”€â”€ src/                      # æºä»£ç 
â”‚   â”œâ”€â”€ models/              # æ¨¡å‹ç®¡ç†å’Œå¹¶è¡Œç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ model_manager.py # æ¨¡å‹ç®¡ç†å™¨
â”‚   â”‚   â””â”€â”€ parallel_strategy.py # å¹¶è¡Œç­–ç•¥å®ç°
â”‚   â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ performance_monitor.py # æ€§èƒ½ç›‘æ§
â”‚   â”‚   â””â”€â”€ device_utils.py  # è®¾å¤‡å·¥å…·
â”‚   â”œâ”€â”€ inference/           # æ¨ç†æ¨¡å—
â”‚   â”œâ”€â”€ evaluation/          # è¯„ä¼°æ¨¡å—
â”‚   â””â”€â”€ data_generation/     # æ•°æ®ç”Ÿæˆæ¨¡å—
â”œâ”€â”€ scripts/                  # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ improved_distributed_launcher.py # åˆ†å¸ƒå¼å¯åŠ¨å™¨
â”‚   â”œâ”€â”€ fixed_distributed_worker.py      # åˆ†å¸ƒå¼å·¥ä½œè¿›ç¨‹
â”‚   â”œâ”€â”€ comprehensive_performance_analysis.py # æ€§èƒ½åˆ†æ
â”‚   â””â”€â”€ comprehensive_performance_report.py   # æŠ¥å‘Šç”Ÿæˆ
â”œâ”€â”€ results/                  # æµ‹è¯•ç»“æœ
â”‚   â”œâ”€â”€ *.json               # åŸå§‹æ€§èƒ½æ•°æ®
â”‚   â”œâ”€â”€ *.md                 # æ€§èƒ½æŠ¥å‘Š
â”‚   â””â”€â”€ *.png                # æ€§èƒ½å›¾è¡¨
â””â”€â”€ logs/                     # æ—¥å¿—æ–‡ä»¶
```

## ğŸ”¬ å¹¶è¡Œç­–ç•¥è¯´æ˜

### 1. Pure Data Parallel (çº¯æ•°æ®å¹¶è¡Œ)

- **åŸç†**: å°†æ•°æ®åˆ†ç‰‡åˆ°ä¸åŒ GPUï¼Œæ¯ä¸ª GPU è¿è¡Œå®Œæ•´æ¨¡å‹
- **é€‚ç”¨åœºæ™¯**: æ¨¡å‹èƒ½å®Œå…¨æ”¾å…¥å• GPU å†…å­˜
- **é€šä¿¡æ¨¡å¼**: AllReduce åŒæ­¥æ¢¯åº¦
- **æ€§èƒ½**: çº¿æ€§æ‰©å±•ï¼Œé€šä¿¡å¼€é”€å°

### 2. Tensor Data Hybrid (å¼ é‡æ•°æ®æ··åˆå¹¶è¡Œ)

- **åŸç†**: ç»“åˆå¼ é‡å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ
- **å¼ é‡å¹¶è¡Œ**: å°†æ³¨æ„åŠ›å¤´åˆ†ç‰‡åˆ°ä¸åŒ GPU
- **æ•°æ®å¹¶è¡Œ**: åœ¨å¼ é‡å¹¶è¡Œç»„é—´è¿›è¡Œæ•°æ®å¹¶è¡Œ
- **é€‚ç”¨åœºæ™¯**: å¤§æ¨¡å‹éœ€è¦å¼ é‡å¹¶è¡Œé™ä½å• GPU å†…å­˜

### 3. Pipeline Data Hybrid (æµæ°´çº¿æ•°æ®æ··åˆå¹¶è¡Œ)

- **åŸç†**: ç»“åˆæµæ°´çº¿å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ
- **æµæ°´çº¿å¹¶è¡Œ**: å°†æ¨¡å‹å±‚åˆ†ç‰‡åˆ°ä¸åŒ GPU
- **æ•°æ®å¹¶è¡Œ**: åœ¨æµæ°´çº¿å¹¶è¡Œç»„é—´è¿›è¡Œæ•°æ®å¹¶è¡Œ
- **é€‚ç”¨åœºæ™¯**: æå¤§æ¨¡å‹éœ€è¦æµæ°´çº¿å¹¶è¡Œ

### 4. Full Model Parallel (å…¨æ¨¡å‹å¹¶è¡Œ)

- **åŸç†**: åŒæ—¶ä½¿ç”¨å¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œ
- **å¤æ‚åº¦**: æœ€é«˜ï¼Œéœ€è¦ç²¾ç»†çš„é€šä¿¡åè°ƒ
- **æ€§èƒ½**: åœ¨å¤§è§„æ¨¡éƒ¨ç½²æ—¶è¡¨ç°æœ€ä½³
- **é€‚ç”¨åœºæ™¯**: è¶…å¤§æ¨¡å‹å’Œå¤§è§„æ¨¡ GPU é›†ç¾¤

## ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ

åŸºäº RTX 3080 GPU çš„å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœï¼š

| é…ç½®                      | æ€»ååé‡ (tokens/sec) | åŠ é€Ÿæ¯” | å¹¶è¡Œæ•ˆç‡ | å¹³å‡å»¶è¿Ÿ (s) | GPU åˆ©ç”¨ç‡ |
| ------------------------- | --------------------- | ------ | -------- | ------------ | ---------- |
| 1GPU Pure Data Parallel   | 27.46                 | 1.00x  | 100%     | 3.88         | 94.7%      |
| 2GPU Pure Data Parallel   | 46.00                 | 1.68x  | 83.8%    | 2.31         | 85.2%      |
| 2GPU Tensor Data Hybrid   | 46.10                 | 1.68x  | 84.0%    | 2.30         | 85.1%      |
| 2GPU Pipeline Data Hybrid | 45.95                 | 1.67x  | 83.7%    | 2.32         | 85.3%      |
| 2GPU Full Model Parallel  | 46.05                 | 1.68x  | 83.9%    | 2.31         | 85.0%      |
| 4GPU Pure Data Parallel   | 69.20                 | 2.52x  | 63.0%    | 1.54         | 68.5%      |
| 4GPU Tensor Data Hybrid   | 69.75                 | 2.54x  | 63.5%    | 1.53         | 68.8%      |
| 4GPU Pipeline Data Hybrid | 69.60                 | 2.53x  | 63.3%    | 1.53         | 68.7%      |
| 4GPU Full Model Parallel  | 70.69                 | 2.57x  | 64.3%    | 1.51         | 69.2%      |

### ğŸ† å…³é”®å‘ç°

- **æœ€ä½³é…ç½®**: 4GPU Full Model Parallel (70.69 tokens/sec, 2.57x åŠ é€Ÿæ¯”)
- **çº¿æ€§æ‰©å±•**: 2GPU é…ç½®å®ç° 83.8%å¹³å‡å¹¶è¡Œæ•ˆç‡
- **å†…å­˜ä¼˜åŒ–**: æˆåŠŸåœ¨ 10GB æ˜¾å­˜é™åˆ¶ä¸‹è¿è¡Œ GPT2-XL (1.5B å‚æ•°)
- **é€šä¿¡æ•ˆç‡**: ä¸åŒå¹¶è¡Œç­–ç•¥æ€§èƒ½å·®å¼‚å°äº 2%ï¼Œè¯´æ˜é€šä¿¡å¼€é”€æ§åˆ¶è‰¯å¥½

## ğŸ“ˆ æµ‹è¯•è§„æ ¼

- **æ¨¡å‹**: GPT2-XL (1.5B å‚æ•°)
- **æµ‹è¯•æ ·æœ¬**: 60 ä¸ª (æ¯ä¸ª rank 20 ä¸ªæ ·æœ¬ Ã— 3 æ¬¡è¿­ä»£)
- **æ‰¹æ¬¡å¤§å°**: 4
- **åºåˆ—é•¿åº¦**: è¾“å…¥ 256 tokens, è¾“å‡º 64 tokens
- **GPU é…ç½®**: 1GPU, 2GPU, 4GPU
- **å¹¶è¡Œç­–ç•¥**: Pure Data Parallel, Tensor Data Hybrid, Pipeline Data Hybrid, Full Model Parallel
- **æµ‹è¯•ç¯å¢ƒ**: RTX 3080 (10GB VRAM) Ã— 4

## ğŸš€ è¯¦ç»†ä½¿ç”¨æŒ‡å—

### æ­¥éª¤ 1: ç¯å¢ƒå‡†å¤‡

#### 1.1 ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, Linux Ubuntu 18.04+
- **Python**: 3.8 - 3.11 (æ¨è 3.9)
- **GPU**: NVIDIA RTX ç³»åˆ— (RTX 3080/4080/4090 ç­‰)ï¼Œæ˜¾å­˜ â‰¥ 8GB
- **CUDA**: 11.8+ æˆ– 12.0+
- **ç£ç›˜ç©ºé—´**: â‰¥ 10GB (ç”¨äºæ¨¡å‹ã€æ•°æ®å’Œç»“æœ)

#### 1.2 å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd gpt_inference_zbz

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# Windows CMD:
.\venv\Scripts\activate.bat
# Linux/Mac:
source venv/bin/activate

# å®‰è£…ä¾èµ–åŒ…
pip install -r requirements.txt

# éªŒè¯PyTorch CUDAæ”¯æŒ
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device count: {torch.cuda.device_count()}')"
```

#### 1.3 GPU ç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥GPUçŠ¶æ€å’Œå†…å­˜
nvidia-smi

# è¿è¡ŒGPUè¯Šæ–­å·¥å…·
python diagnose_gpu.py

# é¢„æœŸè¾“å‡ºç¤ºä¾‹:
# âœ“ æ£€æµ‹åˆ° 4 ä¸ª GPU è®¾å¤‡
# âœ“ GPU 0: NVIDIA GeForce RTX 3080 (10.0GB)
# âœ“ PyTorch CUDA æ”¯æŒ: æ˜¯
# âœ“ æ‰€æœ‰ GPU å¯æ­£å¸¸è®¿é—®
```

### æ­¥éª¤ 2: å‡†å¤‡æµ‹è¯•æ•°æ®

#### 2.1 ä½¿ç”¨é¢„ç½®æµ‹è¯•æ•°æ® (æ¨èæ–°æ‰‹)

```bash
# éªŒè¯é¢„ç½®æµ‹è¯•æ•°æ®
python validate_dataset.py

# æŸ¥çœ‹æµ‹è¯•æ•°æ®æ ·æœ¬
head -n 3 data/test_prompts.jsonl
```

#### 2.2 ç”Ÿæˆè‡ªå®šä¹‰æµ‹è¯•æ•°æ®

```bash
# ç”Ÿæˆä¸åŒé…ç½®çš„æµ‹è¯•æ•°æ®é›†
python scripts/generate_dataset.py --num_samples 50 --output data/custom_test.jsonl

# ç”ŸæˆåŸºå‡†æµ‹è¯•ç”¨çš„æ•°æ®é›†
python scripts/generate_dataset.py --benchmark --num_configs 6
```

### æ­¥éª¤ 3: å•ç­–ç•¥æ€§èƒ½æµ‹è¯•

#### 3.1 1GPU æµ‹è¯• (å…¥é—¨æµ‹è¯•)

```bash
# åŸºç¡€å•GPUæµ‹è¯•
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 1 \
    --num_samples 20 \
    --batch_size 4 \
    --verbose

# é¢„æœŸè¾“å‡º:
# æ­£åœ¨åˆå§‹åŒ– 1GPU Pure Data Parallel æ¨ç†...
# åŠ è½½æ¨¡å‹: gpt2-xl (1.5B å‚æ•°)
# å¼€å§‹æ¨ç†æµ‹è¯•...
# æ€§èƒ½ç»“æœ: 27.46 tokens/sec, å»¶è¿Ÿ: 3.88s, GPUåˆ©ç”¨ç‡: 94.7%
```

#### 3.2 2GPU åˆ†å¸ƒå¼æµ‹è¯•

```bash
# çº¯æ•°æ®å¹¶è¡Œ
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --num_samples 20

# å¼ é‡æ•°æ®æ··åˆå¹¶è¡Œ
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --num_samples 20

# æµæ°´çº¿æ•°æ®æ··åˆå¹¶è¡Œ
python scripts/improved_distributed_launcher.py \
    --strategy pipeline_data_hybrid \
    --num_gpus 2 \
    --num_samples 20

# å…¨æ¨¡å‹å¹¶è¡Œ
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 2 \
    --num_samples 20
```

#### 3.3 4GPU é«˜æ€§èƒ½æµ‹è¯•

```bash
# 4GPU å…¨æ¨¡å‹å¹¶è¡Œ (æœ€ä½³æ€§èƒ½é…ç½®)
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --num_samples 20 \
    --batch_size 4

# æœŸå¾…ç»“æœ: ~70 tokens/sec, 2.57x åŠ é€Ÿæ¯”
```

#### 3.4 é«˜çº§å‚æ•°é…ç½®

```bash
# è‡ªå®šä¹‰é…ç½®æµ‹è¯•
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --num_samples 50 \
    --batch_size 8 \
    --max_length 512 \
    --temperature 0.8 \
    --top_p 0.9 \
    --repetition_penalty 1.1 \
    --timeout 1800 \
    --output_dir results/custom_test
```

### æ­¥éª¤ 4: å®Œæ•´åŸºå‡†æµ‹è¯•

#### 4.1 è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†

```bash
# è¿è¡Œæ‰€æœ‰ 9 ç§é…ç½®ç»„åˆçš„å®Œæ•´æµ‹è¯•
# (1Ã—1GPU + 4Ã—2GPU + 4Ã—4GPU)
python run_complete_gpu_benchmark.py

# æµ‹è¯•è¿‡ç¨‹ (çº¦30-60åˆ†é’Ÿ):
# [1/9] 1GPU Pure Data Parallel...     âœ“ å®Œæˆ
# [2/9] 2GPU Pure Data Parallel...     âœ“ å®Œæˆ
# [3/9] 2GPU Tensor Data Hybrid...     âœ“ å®Œæˆ
# [4/9] 2GPU Pipeline Data Hybrid...   âœ“ å®Œæˆ
# [5/9] 2GPU Full Model Parallel...    âœ“ å®Œæˆ
# [6/9] 4GPU Pure Data Parallel...     âœ“ å®Œæˆ
# [7/9] 4GPU Tensor Data Hybrid...     âœ“ å®Œæˆ
# [8/9] 4GPU Pipeline Data Hybrid...   âœ“ å®Œæˆ
# [9/9] 4GPU Full Model Parallel...    âœ“ å®Œæˆ
```

#### 4.2 ç›‘æ§æµ‹è¯•è¿›åº¦

```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f logs/distributed_inference.log
```

### æ­¥éª¤ 5: ç»“æœåˆ†æ

#### 5.1 ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š

```bash
# ç”Ÿæˆç»¼åˆæ€§èƒ½åˆ†æ (å›¾è¡¨ + æ•°æ®)
python scripts/comprehensive_performance_analysis.py

# è¾“å‡ºæ–‡ä»¶:
# - analysis_reports/throughput_comparison.png      # ååé‡å¯¹æ¯”å›¾
# - analysis_reports/latency_comparison.png         # å»¶è¿Ÿå¯¹æ¯”å›¾
# - analysis_reports/parallel_efficiency.png        # å¹¶è¡Œæ•ˆç‡å›¾
# - analysis_reports/avg_throughput_per_gpu.png     # å•GPUæ€§èƒ½å›¾
# - analysis_reports/performance_summary.csv        # æ€§èƒ½æ•°æ®è¡¨
# - analysis_reports/performance_report.txt         # è¯¦ç»†æ–‡æœ¬æŠ¥å‘Š
```

#### 5.2 ç”Ÿæˆè¯¦ç»†æ€§èƒ½æŠ¥å‘Š

```bash
# ç”Ÿæˆ Markdown æ ¼å¼çš„ç»¼åˆæŠ¥å‘Š
python scripts/comprehensive_performance_report.py

# è¾“å‡º: results/comprehensive_performance_report_YYYYMMDD_HHMMSS.md
```

#### 5.3 æŸ¥çœ‹å’Œåˆ†æç»“æœ

```bash
# æŸ¥çœ‹æ€§èƒ½æ‘˜è¦
cat analysis_reports/performance_report.txt

# æŸ¥çœ‹è¯¦ç»†æ•°æ®
head -n 20 analysis_reports/performance_summary.csv

# åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹å›¾è¡¨
start analysis_reports/throughput_comparison.png        # Windows
open analysis_reports/throughput_comparison.png         # macOS
xdg-open analysis_reports/throughput_comparison.png     # Linux
```

### æ­¥éª¤ 6: é«˜çº§ä½¿ç”¨

#### 6.1 è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

ç¼–è¾‘é…ç½®æ–‡ä»¶ä»¥é€‚åº”æ‚¨çš„éœ€æ±‚ï¼š

```bash
# ç¼–è¾‘æ¨¡å‹é…ç½®
notepad config/model_config.yaml        # Windows
vim config/model_config.yaml            # Linux/Mac

# å…³é”®é…ç½®é¡¹:
# model_name: "gpt2-xl"                  # æ¨¡å‹åç§°
# max_position_embeddings: 1024         # æœ€å¤§åºåˆ—é•¿åº¦
# torch_dtype: "float16"                # æ•°æ®ç±»å‹
# device_map: "auto"                    # è®¾å¤‡æ˜ å°„ç­–ç•¥
```

#### 6.2 æ‰¹é‡æµ‹è¯•è„šæœ¬

```bash
# åˆ›å»ºæ‰¹é‡æµ‹è¯•è„šæœ¬
cat > batch_test.sh << 'EOF'
#!/bin/bash
strategies=("pure_data_parallel" "tensor_data_hybrid" "pipeline_data_hybrid" "full_model_parallel")
gpus=(1 2 4)

for strategy in "${strategies[@]}"; do
    for gpu in "${gpus[@]}"; do
        echo "Testing $strategy with $gpu GPU(s)..."
        python scripts/improved_distributed_launcher.py \
            --strategy $strategy \
            --num_gpus $gpu \
            --num_samples 10 \
            --output_dir results/batch_$strategy_$gpu
    done
done
EOF

chmod +x batch_test.sh
./batch_test.sh
```

#### 6.3 æ€§èƒ½è°ƒä¼˜

```bash
# GPU å†…å­˜ä¼˜åŒ–æµ‹è¯•
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --num_samples 20 \
    --enable_memory_efficient_attention \
    --gradient_checkpointing \
    --mixed_precision

# å¤§æ‰¹æ¬¡å¤§å°æµ‹è¯• (å¦‚æœæ˜¾å­˜å……è¶³)
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 4 \
    --num_samples 20 \
    --batch_size 16 \
    --max_length 1024
```

### æ­¥éª¤ 7: å¸¸è§ä½¿ç”¨åœºæ™¯

#### 7.1 æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```bash
# å¯¹æ¯”ä¸åŒå¹¶è¡Œç­–ç•¥åœ¨2GPUé…ç½®ä¸‹çš„æ€§èƒ½
for strategy in pure_data_parallel tensor_data_hybrid pipeline_data_hybrid full_model_parallel; do
    echo "Testing $strategy..."
    python scripts/improved_distributed_launcher.py \
        --strategy $strategy \
        --num_gpus 2 \
        --num_samples 20 \
        --output_dir results/comparison_$strategy
done

# åˆ†æå¯¹æ¯”ç»“æœ
python scripts/comprehensive_performance_analysis.py
```

#### 7.2 æ‰©å±•æ€§æµ‹è¯•

```bash
# æµ‹è¯•ç›¸åŒç­–ç•¥åœ¨ä¸åŒGPUæ•°é‡ä¸‹çš„æ‰©å±•æ€§
strategy="full_model_parallel"
for gpus in 1 2 4; do
    echo "Testing $strategy with $gpus GPU(s)..."
    python scripts/improved_distributed_launcher.py \
        --strategy $strategy \
        --num_gpus $gpus \
        --num_samples 20 \
        --output_dir results/scaling_$strategy_$gpus
done
```

#### 7.3 ç”Ÿäº§ç¯å¢ƒæ€§èƒ½è¯„ä¼°

```bash
# æ¨¡æ‹Ÿç”Ÿäº§è´Ÿè½½æµ‹è¯•
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --num_samples 100 \
    --batch_size 8 \
    --max_length 512 \
    --temperature 0.7 \
    --repetition_penalty 1.05 \
    --output_dir results/production_test \
    --save_detailed_metrics
```

## ğŸ“‹ æ ¸å¿ƒæ¨¡å—è¯´æ˜

### ModelManager (`src/models/model_manager.py`)

- ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½å’Œç®¡ç†æ¥å£
- æ”¯æŒä¸åŒå¹¶è¡Œç­–ç•¥çš„æ¨¡å‹åˆå§‹åŒ–
- è‡ªåŠ¨å†…å­˜ä¼˜åŒ–å’Œè®¾å¤‡æ˜ å°„

### ParallelStrategy (`src/models/parallel_strategy.py`)

- å››ç§å¹¶è¡Œç­–ç•¥çš„å…·ä½“å®ç°
- è¿›ç¨‹ç»„ç®¡ç†å’Œé€šä¿¡åè°ƒ
- è‡ªé€‚åº”é…ç½®ä¼˜åŒ–

### PerformanceMonitor (`src/utils/performance_monitor.py`)

- å®æ—¶æ€§èƒ½æŒ‡æ ‡ç›‘æ§
- GPU åˆ©ç”¨ç‡å’Œå†…å­˜ä½¿ç”¨ç»Ÿè®¡
- åˆ†å¸ƒå¼æ€§èƒ½æ•°æ®èšåˆ

### DistributedLauncher (`scripts/improved_distributed_launcher.py`)

- åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨
- è‡ªåŠ¨è¿›ç¨‹ç®¡ç†å’Œå®¹é”™å¤„ç†
- æ”¯æŒå¤šç§å¹¶è¡Œç­–ç•¥é…ç½®

## ğŸ”§ è¯¦ç»†æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜è§£å†³

#### é—®é¢˜ 1: CUDA è®¾å¤‡ä¸å¯è§

**ç—‡çŠ¶**:

```
RuntimeError: No CUDA devices available
CUDA_VISIBLE_DEVICES shows no devices
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAå®‰è£…
nvidia-smi
nvcc --version

# 2. æ£€æŸ¥PyTorch CUDAæ”¯æŒ
python -c "import torch; print(torch.cuda.is_available())"

# 3. è®¾ç½®å¯è§GPUç¯å¢ƒå˜é‡
# Windows PowerShell:
$env:CUDA_VISIBLE_DEVICES="0,1,2,3"
# Windows CMD:
set CUDA_VISIBLE_DEVICES=0,1,2,3
# Linux/Mac:
export CUDA_VISIBLE_DEVICES=0,1,2,3

# 4. é‡æ–°è¿è¡ŒGPUè¯Šæ–­
python diagnose_gpu.py
```

#### é—®é¢˜ 2: GPU å†…å­˜ä¸è¶³ (CUDA OOM)

**ç—‡çŠ¶**:

```
RuntimeError: CUDA out of memory. Tried to allocate X.XXGiB
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ–¹æ¡ˆ1: å‡å°‘æ‰¹æ¬¡å¤§å°
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --batch_size 2 \
    --num_samples 20

# æ–¹æ¡ˆ2: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --gradient_checkpointing \
    --num_samples 20

# æ–¹æ¡ˆ3: ä½¿ç”¨æ··åˆç²¾åº¦
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --mixed_precision \
    --num_samples 20

# æ–¹æ¡ˆ4: å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
python scripts/improved_distributed_launcher.py \
    --strategy pipeline_data_hybrid \
    --num_gpus 2 \
    --enable_memory_efficient_attention \
    --num_samples 20
```

#### é—®é¢˜ 3: åˆ†å¸ƒå¼é€šä¿¡è¶…æ—¶

**ç—‡çŠ¶**:

```
RuntimeError: ProcessGroup timeout
torch.distributed initialization failed
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --timeout 3600 \
    --num_samples 20

# 2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™
# Windowsé˜²ç«å¢™è®¾ç½®å…è®¸Python.exe
# æˆ–æš‚æ—¶å…³é—­é˜²ç«å¢™è¿›è¡Œæµ‹è¯•

# 3. ä½¿ç”¨å•æœºæµ‹è¯•æ¨¡å¼
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 1 \
    --num_samples 10

# 4. é‡ç½®PyTorchåˆ†å¸ƒå¼åç«¯
export NCCL_DEBUG=INFO  # Linux
$env:NCCL_DEBUG="INFO"  # Windows PowerShell
```

#### é—®é¢˜ 4: æ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**:

```
OSError: Can't load config/model from 'gpt2-xl'
ConnectionError: Couldn't reach huggingface.co
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ (é¦–æ¬¡è¿è¡Œ)
python -c "
from transformers import GPT2LMHeadModel, GPT2Tokenizer
model = GPT2LMHeadModel.from_pretrained('gpt2-xl')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
print('æ¨¡å‹ä¸‹è½½å®Œæˆ')
"

# 2. è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
# Windows:
$env:HF_HOME="D:\cache\huggingface"
# Linux:
export HF_HOME="/path/to/huggingface/cache"

# 3. ä½¿ç”¨ç¦»çº¿æ¨¡å¼
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 1 \
    --offline_mode \
    --num_samples 10
```

#### é—®é¢˜ 5: è¿›ç¨‹å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶**:

```
torch.multiprocessing spawn failed
ProcessGroup initialization error
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ¸…ç†ç°æœ‰è¿›ç¨‹
# Windows:
taskkill /f /im python.exe
# Linux:
pkill -f python

# 2. æ£€æŸ¥ç«¯å£å ç”¨
netstat -ano | findstr :29500  # Windows
lsof -i :29500                 # Linux

# 3. ä½¿ç”¨ä¸åŒç«¯å£
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --master_port 29501 \
    --num_samples 20

# 4. é‡å¯ç³»ç»Ÿä»¥æ¸…ç†èµ„æº
```

#### é—®é¢˜ 6: æƒé™é”™è¯¯

**ç—‡çŠ¶**:

```
PermissionError: Access denied
OSError: Cannot write to results directory
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. æ£€æŸ¥ç›®å½•æƒé™
# Windows:
icacls results
mkdir results 2>nul

# Linux:
ls -la results/
chmod 755 results/

# 2. ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ (Windows)
# å³é”®ç‚¹å‡»PowerShell -> "ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ"

# 3. æ›´æ”¹è¾“å‡ºç›®å½•
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 1 \
    --output_dir C:\temp\gpu_results \
    --num_samples 10
```

### è°ƒè¯•å·¥å…·å’Œæ–¹æ³•

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
# å¯ç”¨è¯¦ç»†è¾“å‡º
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --verbose \
    --debug \
    --num_samples 10

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
tail -f logs/distributed_inference.log  # Linux
Get-Content logs\distributed_inference.log -Wait  # Windows PowerShell
```

#### GPU å†…å­˜ç›‘æ§

```bash
# å®æ—¶ç›‘æ§GPUçŠ¶æ€
# Linux:
watch -n 1 nvidia-smi

# Windows PowerShell:
while ($true) { clear; nvidia-smi; Start-Sleep 1 }

# ç›‘æ§ç‰¹å®šGPU
nvidia-smi -i 0 -l 1  # ç›‘æ§GPU 0

# æŸ¥çœ‹è¿›ç¨‹è¯¦æƒ…
nvidia-smi pmon -i 0 -d 1  # è¿›ç¨‹ç›‘æ§
```

#### æ€§èƒ½åˆ†æå·¥å…·

```bash
# ä½¿ç”¨PyTorch Profiler
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --enable_profiling \
    --num_samples 5

# åˆ†æProfileç»“æœ
python -c "
import torch
profile = torch.load('results/pytorch_profile.pt')
print(profile.key_averages().table(sort_by='cuda_time_total'))
"
```

#### ç¯å¢ƒæ£€æŸ¥è„šæœ¬

```bash
# åˆ›å»ºå®Œæ•´ç¯å¢ƒæ£€æŸ¥è„šæœ¬
cat > check_environment.py << 'EOF'
import torch
import transformers
import sys
import os

print("=== ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥ ===")
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"    å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB")

print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')}")
print("=== æ£€æŸ¥å®Œæˆ ===")
EOF

python check_environment.py
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### é’ˆå¯¹ RTX 3080 çš„ä¼˜åŒ–

```bash
# RTX 3080 (10GB) æ¨èé…ç½®
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --batch_size 4 \
    --max_length 256 \
    --mixed_precision \
    --enable_memory_efficient_attention \
    --num_samples 20
```

#### é’ˆå¯¹ RTX 4090 çš„ä¼˜åŒ–

```bash
# RTX 4090 (24GB) é«˜æ€§èƒ½é…ç½®
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --batch_size 8 \
    --max_length 512 \
    --num_samples 50
```

#### é’ˆå¯¹å¤š GPU æœåŠ¡å™¨çš„ä¼˜åŒ–

```bash
# 8GPU å¤§è§„æ¨¡éƒ¨ç½²é…ç½®
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 8 \
    --batch_size 16 \
    --max_length 1024 \
    --gradient_checkpointing \
    --num_samples 100
```

## â“ å¸¸è§é—®é¢˜ FAQ

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ GPU åˆ©ç”¨ç‡å¾ˆä½ï¼Ÿ

**A**: GPU åˆ©ç”¨ç‡ä½å¯èƒ½ç”±ä»¥ä¸‹åŸå› é€ æˆï¼š

1. **æ‰¹æ¬¡å¤§å°å¤ªå°**: å¢åŠ  `--batch_size` å‚æ•°
2. **åºåˆ—é•¿åº¦å¤ªçŸ­**: å¢åŠ  `--max_length` å‚æ•°
3. **CPU ç“¶é¢ˆ**: æ£€æŸ¥ CPU ä½¿ç”¨ç‡ï¼Œå¢åŠ æ•°æ®é¢„å¤„ç†çº¿ç¨‹
4. **å†…å­˜å¸¦å®½é™åˆ¶**: ä½¿ç”¨æ··åˆç²¾åº¦ `--mixed_precision`

```bash
# ä¼˜åŒ–GPUåˆ©ç”¨ç‡çš„é…ç½®ç¤ºä¾‹
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --batch_size 8 \
    --max_length 512 \
    --mixed_precision \
    --num_samples 20
```

### Q2: ä¸åŒå¹¶è¡Œç­–ç•¥çš„é€‰æ‹©å»ºè®®æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: é€‰æ‹©ç­–ç•¥ä¾æ®ï¼š

- **1-2 GPU**: æ¨è `pure_data_parallel`ï¼Œç®€å•é«˜æ•ˆ
- **2-4 GPU**: æ¨è `tensor_data_hybrid`ï¼Œå¹³è¡¡æ€§èƒ½å’Œå¤æ‚åº¦
- **4+ GPU**: æ¨è `full_model_parallel`ï¼Œæœ€å¤§åŒ–åˆ©ç”¨å¤š GPU
- **æ˜¾å­˜ä¸è¶³**: æ¨è `pipeline_data_hybrid`ï¼Œæœ€çœæ˜¾å­˜

### Q3: å¦‚ä½•è§£é‡Šæ€§èƒ½æŒ‡æ ‡ï¼Ÿ

**A**: å…³é”®æŒ‡æ ‡è¯´æ˜ï¼š

- **æ€»ååé‡** (tokens/sec): ç³»ç»Ÿæ•´ä½“å¤„ç†é€Ÿåº¦ï¼Œè¶Šé«˜è¶Šå¥½
- **åŠ é€Ÿæ¯”**: ç›¸å¯¹äº 1GPU çš„æ€§èƒ½æå‡å€æ•°ï¼Œç†æƒ³å€¼ä¸º GPU æ•°é‡
- **å¹¶è¡Œæ•ˆç‡**: åŠ é€Ÿæ¯”/GPU æ•°é‡ï¼Œåæ˜ èµ„æºåˆ©ç”¨æ•ˆç‡ï¼Œ>80%ä¸ºä¼˜ç§€
- **å¹³å‡å»¶è¿Ÿ**: å•ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´ï¼Œè¶Šä½è¶Šå¥½
- **GPU åˆ©ç”¨ç‡**: GPU è®¡ç®—èµ„æºä½¿ç”¨ç™¾åˆ†æ¯”ï¼Œ>90%ä¸ºä¼˜ç§€

### Q4: æµ‹è¯•ç»“æœä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

**A**: æé«˜æµ‹è¯•ç¨³å®šæ€§çš„æ–¹æ³•ï¼š

```bash
# 1. å¢åŠ æµ‹è¯•æ ·æœ¬æ•°é‡
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 2 \
    --num_samples 50 \
    --num_iterations 5

# 2. å¯ç”¨é¢„çƒ­ (warm-up)
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 2 \
    --warmup_steps 10 \
    --num_samples 30

# 3. å›ºå®šéšæœºç§å­
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --seed 42 \
    --num_samples 20
```

### Q5: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Ÿ

**A**: ç”Ÿäº§éƒ¨ç½²å»ºè®®ï¼š

1. **é€‰æ‹©æœ€ä½³é…ç½®**: æ ¹æ®åŸºå‡†æµ‹è¯•ç»“æœé€‰æ‹©æœ€ä¼˜ç­–ç•¥
2. **èµ„æºç›‘æ§**: å®æ–½ GPUã€å†…å­˜ã€ç½‘ç»œç›‘æ§
3. **è´Ÿè½½å‡è¡¡**: é…ç½®è¯·æ±‚åˆ†å‘å’Œé˜Ÿåˆ—ç®¡ç†
4. **å®¹é”™å¤„ç†**: å®ç°è‡ªåŠ¨é‡å¯å’Œæ•…éšœæ¢å¤
5. **æ€§èƒ½è°ƒä¼˜**: æ ¹æ®å®é™…è´Ÿè½½è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå¹¶å‘æ•°

### Q6: æ”¯æŒå“ªäº›æ¨¡å‹ï¼Ÿ

**A**: å½“å‰æ”¯æŒçš„æ¨¡å‹ï¼š

- **GPT-2 ç³»åˆ—**: gpt2, gpt2-medium, gpt2-large, gpt2-xl
- **æ‰©å±•æ”¯æŒ**: ç³»ç»Ÿè®¾è®¡æ”¯æŒå…¶ä»– Transformer æ¨¡å‹
- **è‡ªå®šä¹‰æ¨¡å‹**: å¯é€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹

ä¿®æ”¹ `config/model_config.yaml` ä»¥æ”¯æŒå…¶ä»–æ¨¡å‹ï¼š

```yaml
model_name: "your-custom-model"
model_type: "gpt2" # æˆ–å…¶ä»–æ”¯æŒçš„ç±»å‹
torch_dtype: "float16"
device_map: "auto"
```

### Q7: å¦‚ä½•æ·»åŠ æ–°çš„å¹¶è¡Œç­–ç•¥ï¼Ÿ

**A**: æ·»åŠ è‡ªå®šä¹‰å¹¶è¡Œç­–ç•¥ï¼š

1. åœ¨ `src/models/parallel_strategy.py` ä¸­æ·»åŠ æ–°çš„ç­–ç•¥ç±»
2. å®ç° `setup_parallel_groups()` å’Œ `get_model_parallel_config()` æ–¹æ³•
3. åœ¨å¯åŠ¨å™¨ä¸­æ³¨å†Œæ–°ç­–ç•¥
4. è¿›è¡Œå……åˆ†æµ‹è¯•éªŒè¯

### Q8: æµ‹è¯•æ•°æ®çš„æ ¼å¼è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ

**A**: æµ‹è¯•æ•°æ®æ ¼å¼ (JSON Lines):

```json
{"prompt": "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ", "max_length": 100}
{"prompt": "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ", "max_length": 150}
{"prompt": "æè¿°æ·±åº¦å­¦ä¹ çš„å‘å±•å†ç¨‹", "max_length": 200}
```

å¿…éœ€å­—æ®µ:

- `prompt`: è¾“å…¥æ–‡æœ¬
- `max_length`: ç”Ÿæˆé•¿åº¦ (å¯é€‰ï¼Œé»˜è®¤ 64)

## ğŸ“‹ å‚æ•°é…ç½®è¯¦è§£

### å¯åŠ¨å™¨å‚æ•°è¯´æ˜

| å‚æ•°                                  | ç±»å‹  | é»˜è®¤å€¼             | è¯´æ˜             |
| ------------------------------------- | ----- | ------------------ | ---------------- |
| `--strategy`                          | str   | pure_data_parallel | å¹¶è¡Œç­–ç•¥         |
| `--num_gpus`                          | int   | 1                  | GPU æ•°é‡         |
| `--num_samples`                       | int   | 20                 | æµ‹è¯•æ ·æœ¬æ•°       |
| `--batch_size`                        | int   | 4                  | æ‰¹æ¬¡å¤§å°         |
| `--max_length`                        | int   | 64                 | æœ€å¤§ç”Ÿæˆé•¿åº¦     |
| `--temperature`                       | float | 1.0                | é‡‡æ ·æ¸©åº¦         |
| `--top_p`                             | float | 1.0                | nucleus é‡‡æ ·é˜ˆå€¼ |
| `--repetition_penalty`                | float | 1.0                | é‡å¤æƒ©ç½š         |
| `--mixed_precision`                   | bool  | False              | æ··åˆç²¾åº¦è®­ç»ƒ     |
| `--gradient_checkpointing`            | bool  | False              | æ¢¯åº¦æ£€æŸ¥ç‚¹       |
| `--enable_memory_efficient_attention` | bool  | False              | å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›   |
| `--timeout`                           | int   | 1800               | è¶…æ—¶æ—¶é—´(ç§’)     |
| `--master_port`                       | int   | 29500              | ä¸»ç«¯å£           |
| `--output_dir`                        | str   | results            | è¾“å‡ºç›®å½•         |
| `--verbose`                           | bool  | False              | è¯¦ç»†è¾“å‡º         |
| `--debug`                             | bool  | False              | è°ƒè¯•æ¨¡å¼         |

### å¹¶è¡Œç­–ç•¥é…ç½®

#### Pure Data Parallel

```bash
python scripts/improved_distributed_launcher.py \
    --strategy pure_data_parallel \
    --num_gpus 4 \
    --batch_size 4 \
    --num_samples 20
```

é€‚ç”¨åœºæ™¯: æ¨¡å‹å¯å®Œå…¨æ”¾å…¥å• GPUï¼Œéœ€è¦æé«˜ååé‡

#### Tensor Data Hybrid

```bash
python scripts/improved_distributed_launcher.py \
    --strategy tensor_data_hybrid \
    --num_gpus 4 \
    --tensor_parallel_size 2 \
    --data_parallel_size 2 \
    --num_samples 20
```

é€‚ç”¨åœºæ™¯: æ¨¡å‹è¾ƒå¤§ï¼Œéœ€è¦å¼ é‡å¹¶è¡Œå‡å°‘å• GPU å†…å­˜

#### Pipeline Data Hybrid

```bash
python scripts/improved_distributed_launcher.py \
    --strategy pipeline_data_hybrid \
    --num_gpus 4 \
    --pipeline_parallel_size 2 \
    --data_parallel_size 2 \
    --num_samples 20
```

é€‚ç”¨åœºæ™¯: æ¨¡å‹æå¤§ï¼Œéœ€è¦æµæ°´çº¿å¹¶è¡Œ

#### Full Model Parallel

```bash
python scripts/improved_distributed_launcher.py \
    --strategy full_model_parallel \
    --num_gpus 4 \
    --tensor_parallel_size 2 \
    --pipeline_parallel_size 2 \
    --num_samples 20
```

é€‚ç”¨åœºæ™¯: è¶…å¤§æ¨¡å‹ï¼Œéœ€è¦æ‰€æœ‰å¹¶è¡ŒæŠ€æœ¯

## ğŸ“š é…ç½®è¯´æ˜

ä¸»è¦é…ç½®æ–‡ä»¶ï¼š

- `config/model_config.yaml`: æ¨¡å‹å‚æ•°é…ç½®
- `config/inference_config.yaml`: æ¨ç†å‚æ•°é…ç½®
- `config/deepspeed_config.json`: DeepSpeed ä¼˜åŒ–é…ç½®

è¯¦ç»†é…ç½®é€‰é¡¹è¯·å‚è€ƒå„é…ç½®æ–‡ä»¶ä¸­çš„æ³¨é‡Šè¯´æ˜ã€‚

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚
