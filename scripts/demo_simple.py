"""
ç®€å•çš„åˆ†å¸ƒå¼æ¨ç†æ¼”ç¤ºè„šæœ¬
ç”¨äºå¿«é€Ÿæµ‹è¯•ç³»ç»ŸåŠŸèƒ½
"""

import os
import sys
import time
import torch
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.model_manager import ModelManager
from src.models.parallel_strategy import ParallelStrategyManager
from src.data_generation.dataset_generator import DatasetGenerator

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("="*60)
    print("æµ‹è¯•1: æ¨¡å‹åŠ è½½")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager()
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if torch.cuda.is_available():
            print(f"å‘ç° {torch.cuda.device_count()} å¼ GPU:")
            for i in range(torch.cuda.device_count()):
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("æ²¡æœ‰å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPU")
        
        # åŠ è½½æ¨¡å‹
        print("\\nåŠ è½½æ¨¡å‹...")
        model_manager.load_model()
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = model_manager.get_model_info()
        print(f"æ¨¡å‹ä¿¡æ¯:")
        for key, value in model_info.items():
            print(f"  {key}: {value}")
        
        print("âœ“ æ¨¡å‹åŠ è½½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_parallel_strategies():
    """æµ‹è¯•å¹¶è¡Œç­–ç•¥"""
    print("\\n" + "="*60)
    print("æµ‹è¯•2: å¹¶è¡Œç­–ç•¥")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–ç­–ç•¥ç®¡ç†å™¨
        strategy_manager = ParallelStrategyManager()
        
        # è·å–å¯ç”¨ç­–ç•¥
        strategies = strategy_manager.get_available_strategies()
        print(f"å¯ç”¨çš„å¹¶è¡Œç­–ç•¥:")
        for name, desc in strategies.items():
            print(f"  {name}: {desc}")
        
        # æµ‹è¯•ç­–ç•¥éªŒè¯
        print("\\næµ‹è¯•ç­–ç•¥éªŒè¯:")
        for strategy_name in strategies.keys():
            config = strategy_manager.get_strategy_config(strategy_name)
            is_valid = strategy_manager.validate_strategy(config)
            status = "âœ“" if is_valid else "âœ—"
            print(f"  {status} {strategy_name}: {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")
        
        # æ¨èç­–ç•¥
        recommended = strategy_manager.recommend_strategy(model_size_gb=6.0, batch_size=1)
        print(f"\\næ¨èç­–ç•¥: {recommended}")
        
        print("âœ“ å¹¶è¡Œç­–ç•¥æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— å¹¶è¡Œç­–ç•¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_data_generation():
    """æµ‹è¯•æ•°æ®ç”Ÿæˆ"""
    print("\\n" + "="*60)
    print("æµ‹è¯•3: æ•°æ®ç”Ÿæˆ")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        generator = DatasetGenerator()
        
        # ç”Ÿæˆå°é‡æµ‹è¯•æ•°æ®
        print("ç”Ÿæˆæµ‹è¯•æ•°æ®...")
        test_config = {
            'prompt_lengths': [32, 128],
            'generation_lengths': [32],
            'samples_per_config': 5,
            'data_sources': ['synthetic']  # åªä½¿ç”¨åˆæˆæ•°æ®ä»¥é¿å…ä¸‹è½½
        }
        
        # ç”Ÿæˆæ•°æ®
        dataset = []
        for prompt_len in test_config['prompt_lengths']:
            for gen_len in test_config['generation_lengths']:
                samples = generator.generate_synthetic_samples(
                    num_samples=test_config['samples_per_config'],
                    prompt_length=prompt_len,
                    generation_length=gen_len
                )
                dataset.extend(samples)
        
        print(f"ç”Ÿæˆäº† {len(dataset)} æ¡æµ‹è¯•æ•°æ®")
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_data_dir = Path("data/test")
        test_data_dir.mkdir(parents=True, exist_ok=True)
        
        test_file = test_data_dir / "demo_dataset.jsonl"
        with open(test_file, 'w', encoding='utf-8') as f:
            for sample in dataset:
                f.write(json.dumps(sample, ensure_ascii=False) + '\\n')
        
        print(f"æµ‹è¯•æ•°æ®ä¿å­˜åˆ°: {test_file}")
        print("âœ“ æ•°æ®ç”Ÿæˆæµ‹è¯•é€šè¿‡")
        return True, test_file
        
    except Exception as e:
        print(f"âœ— æ•°æ®ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}")
        return False, None

def test_simple_inference():
    """æµ‹è¯•ç®€å•æ¨ç†"""
    print("\\n" + "="*60)
    print("æµ‹è¯•4: ç®€å•æ¨ç†")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager()
        model_manager.load_model()
        
        # å‡†å¤‡æµ‹è¯•è¾“å…¥
        test_prompts = [
            "The future of artificial intelligence is",
            "In a world where technology advances rapidly",
            "Machine learning algorithms can help us"
        ]
        
        print(f"æµ‹è¯•æ¨ç†ï¼Œå…± {len(test_prompts)} æ¡æ ·æœ¬...")
        
        results = []
        for i, prompt in enumerate(test_prompts):
            print(f"  å¤„ç†æ ·æœ¬ {i+1}/{len(test_prompts)}: {prompt[:50]}...")
            
            start_time = time.time()
            
            # å‡†å¤‡è¾“å…¥
            inputs = model_manager.prepare_inputs([prompt])
            
            # æ‰§è¡Œæ¨ç†
            with torch.no_grad():
                outputs = model_manager.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=32,
                    do_sample=True,
                    temperature=0.8
                )
            
            # è§£ç è¾“å‡º
            generated_text = model_manager.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True
            )
            
            inference_time = time.time() - start_time
            
            # è®¡ç®—tokenæ•°é‡
            prompt_tokens = inputs['input_ids'].shape[1]
            total_tokens = outputs.shape[1]
            generated_tokens = total_tokens - prompt_tokens
            
            result = {
                'prompt': prompt,
                'generated_text': generated_text,
                'prompt_tokens': prompt_tokens,
                'generated_tokens': generated_tokens,
                'inference_time': inference_time,
                'throughput': generated_tokens / inference_time if inference_time > 0 else 0
            }
            
            results.append(result)
            print(f"    ç”Ÿæˆæ–‡æœ¬: {generated_text[len(prompt):].strip()[:100]}...")
            print(f"    æ¨ç†æ—¶é—´: {inference_time:.3f}s, ååé‡: {result['throughput']:.2f} tokens/s")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_time = sum(r['inference_time'] for r in results) / len(results)
        avg_throughput = sum(r['throughput'] for r in results) / len(results)
        
        print(f"\\næ¨ç†æ€§èƒ½æ‘˜è¦:")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}s")
        print(f"  å¹³å‡ååé‡: {avg_throughput:.2f} tokens/s")
        
        print("âœ“ ç®€å•æ¨ç†æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— ç®€å•æ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_monitoring():
    """æµ‹è¯•å†…å­˜ç›‘æ§"""
    print("\\n" + "="*60)
    print("æµ‹è¯•5: å†…å­˜ç›‘æ§")
    print("="*60)
    
    try:
        from src.utils.performance_monitor import PerformanceMonitor
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        monitor = PerformanceMonitor()
        
        print("å¼€å§‹å†…å­˜ç›‘æ§...")
        monitor.start_monitoring()
        
        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
        if torch.cuda.is_available():
            # åˆ›å»ºä¸€äº›å¼ é‡è¿›è¡Œè®¡ç®—
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.mm(x, y)
            
            time.sleep(2)  # ç­‰å¾…2ç§’
            
            # æ¸…ç†
            del x, y, z
            torch.cuda.empty_cache()
        
        time.sleep(1)
        monitor.stop_monitoring()
        
        # è·å–ç›‘æ§ç»“æœ
        stats = monitor.get_resource_stats()
        
        print("å†…å­˜ç›‘æ§ç»“æœ:")
        if 'gpu_memory_usage' in stats:
            gpu_stats = stats['gpu_memory_usage']
            print(f"  GPUæ˜¾å­˜:")
            print(f"    æœ€å¤§ä½¿ç”¨: {max(gpu_stats):.2f} MB")
            print(f"    å¹³å‡ä½¿ç”¨: {sum(gpu_stats)/len(gpu_stats):.2f} MB")
        
        if 'cpu_memory_usage' in stats:
            cpu_stats = stats['cpu_memory_usage']
            print(f"  CPUå†…å­˜:")
            print(f"    æœ€å¤§ä½¿ç”¨: {max(cpu_stats):.2f} MB")
            print(f"    å¹³å‡ä½¿ç”¨: {sum(cpu_stats)/len(cpu_stats):.2f} MB")
        
        print("âœ“ å†…å­˜ç›‘æ§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— å†…å­˜ç›‘æ§æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹GPTåˆ†å¸ƒå¼æ¨ç†ç³»ç»Ÿæ¼”ç¤º")
    print("="*80)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ç¯å¢ƒæ£€æŸ¥:")
    print(f"  Pythonç‰ˆæœ¬: {sys.version}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("æ¨¡å‹åŠ è½½", test_model_loading),
        ("å¹¶è¡Œç­–ç•¥", test_parallel_strategies), 
        ("æ•°æ®ç”Ÿæˆ", test_data_generation),
        ("ç®€å•æ¨ç†", test_simple_inference),
        ("å†…å­˜ç›‘æ§", test_memory_monitoring)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            if test_name == "æ•°æ®ç”Ÿæˆ":
                success, data_file = test_func()
                results[test_name] = success
            else:
                results[test_name] = test_func()
        except KeyboardInterrupt:
            print(f"\\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
            break
        except Exception as e:
            print(f"\\næµ‹è¯• {test_name} å‡ºç°å¼‚å¸¸: {str(e)}")
            results[test_name] = False
    
    # æ€»ç»“ç»“æœ
    print("\\n" + "="*80)
    print("æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results.items():
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
        if success:
            passed += 1
    
    print(f"\\næ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        print("\\nä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œ:")
        print("  python scripts/run_distributed_inference.py --strategy tensor_data_hybrid")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ç›¸å…³é—®é¢˜ã€‚")
    
    print("="*80)

if __name__ == "__main__":
    main()
