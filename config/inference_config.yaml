# 推理配置
inference:
  batch_size: 1 # 推理批次大小
  temperature: 1.0
  top_p: 0.9
  top_k: 50
  do_sample: true
  num_return_sequences: 1
  pad_token_id: 50256
  eos_token_id: 50256

# 性能测试配置
performance:
  warmup_steps: 10 # 预热步数
  num_iterations: 100 # 每个配置的测试迭代次数
  measure_memory: true
  measure_gpu_utilization: true
  measure_communication: true

# 评估指标配置
metrics:
  throughput:
    enabled: true
    unit: "tokens_per_second"

  latency:
    enabled: true
    measure_first_token: true
    measure_total_time: true
    unit: "milliseconds"

  compute_efficiency:
    enabled: true
    measure_gpu_utilization: true
    measure_memory_usage: true
    measure_cpu_usage: true

  communication_overhead:
    enabled: true
    measure_allreduce_time: true
    measure_broadcast_time: true
    measure_p2p_time: true

# 日志配置
logging:
  level: "INFO"
  log_file: "logs/inference_test.log"
  console_output: true
  wandb:
    enabled: false
    project: "gpt-inference-test"
    run_name: "distributed_performance_test"
