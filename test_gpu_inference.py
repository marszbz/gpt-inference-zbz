#!/usr/bin/env python3
"""
GPUæ¨ç†åŠŸèƒ½æµ‹è¯•
éªŒè¯GPUç¯å¢ƒä¸‹çš„æ¨ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šGPUåˆ†å¸ƒå¼æ¨ç†
"""

import sys
import os
import logging
import torch
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    logger = logging.getLogger(__name__)
    logger.info("=== æ£€æŸ¥GPUç¯å¢ƒ ===")
    
    if not torch.cuda.is_available():
        logger.error("CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUæ¨ç†æµ‹è¯•")
        return False
    
    gpu_count = torch.cuda.device_count()
    logger.info(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
    
    for i in range(gpu_count):
        try:
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            logger.info(f"GPU {i}: {gpu_name} - {memory_total:.1f} GB")
            
            # æµ‹è¯•GPUå†…å­˜åˆ†é…
            torch.cuda.set_device(i)
            test_tensor = torch.randn(1000, 1000, device=f'cuda:{i}')
            memory_allocated = torch.cuda.memory_allocated(i) / (1024**2)
            logger.info(f"  å†…å­˜æµ‹è¯•é€šè¿‡ï¼Œå·²åˆ†é…: {memory_allocated:.1f} MB")
            del test_tensor
            torch.cuda.empty_cache()
            
        except Exception as e:
            logger.error(f"GPU {i} æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    return True

def test_model_loading_gpu():
    """æµ‹è¯•GPUä¸Šçš„æ¨¡å‹åŠ è½½"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•GPUæ¨¡å‹åŠ è½½ ===")
    
    try:
        from src.models import ModelManager
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_path = "config/model_config.yaml"
        if not Path(config_path).exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager(config_path)
        logger.info("æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # è®¾ç½®GPUè®¾å¤‡
        device = torch.device("cuda:0")
        model_manager.device = device
        
        logger.info("å‡†å¤‡åŠ è½½æ¨¡å‹åˆ°GPU...")
        logger.info("æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åŠ è½½æ¨¡å‹ï¼ˆè¿™é‡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼‰
        try:
            model_manager.load_model(local_rank=0)
            load_time = time.time() - start_time
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f} ç§’")
            
            # éªŒè¯æ¨¡å‹åœ¨GPUä¸Š
            if hasattr(model_manager.model, 'device'):
                model_device = next(model_manager.model.parameters()).device
                logger.info(f"æ¨¡å‹è®¾å¤‡: {model_device}")
            
            # æµ‹è¯•ç®€å•æ¨ç†
            logger.info("æµ‹è¯•GPUæ¨ç†...")
            test_prompt = "Hello, this is a test prompt for"
            inputs = model_manager.prepare_inputs([test_prompt])
            
            with torch.no_grad():
                outputs = model_manager.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=10,
                    do_sample=True,
                    temperature=0.8
                )
            
            generated_text = model_manager.tokenizer.decode(
                outputs[0], skip_special_tokens=True
            )
            logger.info(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}")
            logger.info("GPUæ¨ç†æµ‹è¯•æˆåŠŸ")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.info("å¯èƒ½çš„åŸå› ï¼š")
            logger.info("1. ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")
            logger.info("2. GPUå†…å­˜ä¸è¶³")
            logger.info("3. æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„é”™è¯¯")
            return False
            
    except Exception as e:
        logger.error(f"GPUæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_memory_management():
    """æµ‹è¯•GPUå†…å­˜ç®¡ç†"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•GPUå†…å­˜ç®¡ç† ===")
    
    try:
        device = torch.device("cuda:0")
        
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        initial_memory = torch.cuda.memory_allocated(0) / (1024**2)
        logger.info(f"åˆå§‹GPUå†…å­˜ä½¿ç”¨: {initial_memory:.1f} MB")
        
        # åˆ†é…ä¸€äº›å†…å­˜
        test_tensors = []
        for i in range(5):
            tensor = torch.randn(1024, 1024, device=device)
            test_tensors.append(tensor)
            current_memory = torch.cuda.memory_allocated(0) / (1024**2)
            logger.info(f"åˆ†é…å¼ é‡ {i+1}ï¼Œå½“å‰å†…å­˜: {current_memory:.1f} MB")
        
        # æ¸…ç†å†…å­˜
        del test_tensors
        torch.cuda.empty_cache()
        
        final_memory = torch.cuda.memory_allocated(0) / (1024**2)
        logger.info(f"æ¸…ç†åGPUå†…å­˜ä½¿ç”¨: {final_memory:.1f} MB")
        
        logger.info("GPUå†…å­˜ç®¡ç†æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"GPUå†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_multi_gpu_detection():
    """æµ‹è¯•å¤šGPUæ£€æµ‹å’Œé…ç½®"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•å¤šGPUæ£€æµ‹ ===")
    
    try:
        gpu_count = torch.cuda.device_count()
        
        if gpu_count == 1:
            logger.info("æ£€æµ‹åˆ°å•GPUç¯å¢ƒ")
            logger.info("å»ºè®®é…ç½®ï¼šä½¿ç”¨æ•°æ®å¹¶è¡Œæˆ–æ¨ç†ä¼˜åŒ–")
        elif gpu_count >= 2:
            logger.info(f"æ£€æµ‹åˆ°å¤šGPUç¯å¢ƒï¼š{gpu_count} ä¸ªGPU")
            logger.info("å¯ç”¨é…ç½®é€‰é¡¹ï¼š")
            logger.info("1. æ•°æ®å¹¶è¡Œ (DataParallel)")
            logger.info("2. åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DistributedDataParallel)")
            logger.info("3. å¼ é‡å¹¶è¡Œ (Tensor Parallelism)")
            logger.info("4. æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)")
            
            # æµ‹è¯•å¤šGPUé€šä¿¡
            if gpu_count >= 2:
                logger.info("æµ‹è¯•å¤šGPUé€šä¿¡...")
                try:
                    tensor1 = torch.randn(100, 100, device='cuda:0')
                    tensor2 = tensor1.to('cuda:1')
                    result = tensor2.sum()
                    logger.info("å¤šGPUé€šä¿¡æµ‹è¯•é€šè¿‡")
                except Exception as e:
                    logger.warning(f"å¤šGPUé€šä¿¡æµ‹è¯•å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        logger.error(f"å¤šGPUæ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_performance_monitoring():
    """æµ‹è¯•GPUæ€§èƒ½ç›‘æ§"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•GPUæ€§èƒ½ç›‘æ§ ===")
    
    try:
        from src.utils import PerformanceMonitor
        
        monitor = PerformanceMonitor()
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = monitor.get_system_info()
        logger.info("ç³»ç»Ÿä¿¡æ¯è·å–æˆåŠŸï¼š")
        logger.info(f"  CPU: {system_info['cpu']['model']}")
        logger.info(f"  CPUæ ¸å¿ƒæ•°: {system_info['cpu']['cores']}")
        logger.info(f"  å†…å­˜: {system_info['memory']['total_gb']:.1f} GB")
        
        if 'gpu' in system_info:
            for i, gpu_info in enumerate(system_info['gpu']):
                logger.info(f"  GPU {i}: {gpu_info['name']} - {gpu_info['memory_total_gb']:.1f} GB")
        
        logger.info("GPUæ€§èƒ½ç›‘æ§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"GPUæ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_inference_engine_gpu():
    """æµ‹è¯•GPUæ¨ç†å¼•æ“"""
    logger = logging.getLogger(__name__)
    logger.info("=== æµ‹è¯•GPUæ¨ç†å¼•æ“ ===")
    
    try:
        from src.models import ModelManager
        from src.inference import DistributedInferenceEngine
        
        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager("config/model_config.yaml")
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        inference_engine = DistributedInferenceEngine(
            model_manager, 
            "config/inference_config.yaml"
        )
        
        logger.info("GPUæ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•GPUç›‘æ§åŠŸèƒ½
        if hasattr(inference_engine, 'measure_gpu_utilization'):
            gpu_util = inference_engine.measure_gpu_utilization()
            logger.info(f"GPUåˆ©ç”¨ç‡ç›‘æ§: {gpu_util}%")
        
        # æµ‹è¯•å†…å­˜ç›‘æ§
        memory_info = inference_engine.measure_memory_usage()
        if 'gpu_memory_allocated_mb' in memory_info:
            logger.info(f"GPUå†…å­˜ç›‘æ§: {memory_info['gpu_memory_allocated_mb']:.1f} MB")
        
        logger.info("GPUæ¨ç†å¼•æ“æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"GPUæ¨ç†å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_gpu_comprehensive_test():
    """è¿è¡ŒGPUç»¼åˆæµ‹è¯•"""
    logger = logging.getLogger(__name__)
    logger.info("å¼€å§‹GPUæ¨ç†åŠŸèƒ½æµ‹è¯•")
    
    # é¦–å…ˆæ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_environment():
        logger.error("GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return False
    
    tests = [
        ("GPUå†…å­˜ç®¡ç†", test_gpu_memory_management),
        ("å¤šGPUæ£€æµ‹", test_multi_gpu_detection),
        ("GPUæ€§èƒ½ç›‘æ§", test_performance_monitoring),
        ("GPUæ¨ç†å¼•æ“", test_inference_engine_gpu),
        ("GPUæ¨¡å‹åŠ è½½", test_model_loading_gpu),  # æ”¾åœ¨æœ€åï¼Œå› ä¸ºå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹
    ]
    
    results = {}
    start_time = time.time()
    
    for test_name, test_func in tests:
        logger.info(f"\n--- å¼€å§‹æµ‹è¯•: {test_name} ---")
        try:
            result = test_func()
            results[test_name] = result
            status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
            logger.info(f"æµ‹è¯•ç»“æœ: {status}")
        except Exception as e:
            results[test_name] = False
            logger.error(f"æµ‹è¯•å¼‚å¸¸: {e}")
    
    end_time = time.time()
    
    # è¾“å‡ºæ€»ç»“
    logger.info("\n" + "="*50)
    logger.info("GPUæ¨ç†åŠŸèƒ½æµ‹è¯•æ€»ç»“")
    logger.info("="*50)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        logger.info(f"{test_name:20s}: {status}")
    
    logger.info(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    logger.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰GPUåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        logger.info("ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„GPUæ¨ç†æ€§èƒ½æµ‹è¯•")
        return True
    else:
        logger.warning(f"âš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False

if __name__ == "__main__":
    setup_logging()
    
    try:
        success = run_gpu_comprehensive_test()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"æµ‹è¯•å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        sys.exit(1)
